[
["index.html", "A Literature Survey of Software Analytics Chapter 1 Preamble 1.1 License", " A Literature Survey of Software Analytics Moritz Beller, IN4334 2018 TU Delft 2018-09-24 Chapter 1 Preamble The book you see in front of you is the outcome of an eight week seminar run by the Software Engineering Research Group (SERG) at TU Delft. We have split up the novel area of Software Analytics into several sub topics. Every chapter addresses one such sub-topic of Software Analytics and is the outcome of a systematic literature review a laborious team of 3-4 students performed. With this book, we hope to structure the new field of Software Analytics and show how it is related to many long existing research fields. Moritz Beller 1.1 License This book is copyrighted 2018 by TU Delft and its respective authors and distributed under a CC BY-NC-SA 4.0 license "],
["a-contemporary-view-on-software-analytics.html", "Chapter 2 A contemporary view on Software Analytics 2.1 What is Software Analytics? 2.2 A list of Software Analytics Sub-Topics", " Chapter 2 A contemporary view on Software Analytics 2.1 What is Software Analytics? 2.2 A list of Software Analytics Sub-Topics "],
["sample-sub-topic.html", "Chapter 3 Sample Sub-Topic 3.1 Motivation 3.2 Research protocol 3.3 Answers", " Chapter 3 Sample Sub-Topic This is an example for the deliverable every group works on. Every group works on one independent chapter (starting as one Rmd file). 3.1 Motivation A short introduction about why the topic you are working on is interesting. The RQs that everyone should be aiming at are: RQ1 Current state of the art in software analytics for your topic : Topics that are being explored Research methods, tools and datasets being used Main research findings, aggregated RQ2 Current state of practice in software analytics for your topic : Tools and companies creating / employing them Case studies and their findings RQ3 Open challenges and future research required 3.2 Research protocol Here, you describe the details of applying Kitchenham’s survey method for your topic, including search queries, fact extraction, coding process and an intial groupping of the papers that you will be analyzing. 3.3 Answers Aggregated answers to the RQs, per RQ. You need: For RQ1 Topics that are being explored Research methods, tools and datasets being used Main research findings, aggregated For RQ2 : Tools and companies creating / employing them Case studies and their findings For RQ3: List of challenges An aggregated set of open research items, as described in the papers Research questions that emerge from the synthesis of the presented works "],
["final-words.html", "Chapter 4 Final Words", " Chapter 4 Final Words We have finished a nice book on Software Analytics. "],
["references.html", "References", " References "],
["app-store-analytics.html", "Chapter 5 App Store analytics 5.1 API change and fault proneness: A threat to the success of Android apps 5.2 The Impact of API Change and Fault-Proneness on the User Ratings of Android Apps 5.3 How can i improve my app? Classifying user reviews for software maintenance and evolution", " Chapter 5 App Store analytics 5.1 API change and fault proneness: A threat to the success of Android apps M. Linares-Vásquez, G. Bavota, C. Bernal-Cárdenas, M. Di Penta, R. Oliveto, and D. Poshyvanyk, in Proceedings of the 2013 9th joint meeting on foundations of software engineering, 2013, pp. 477–487. The paper presents an empirical study that aims to corroborate the relationship between the fault and change-proneness of APIs and the degree of success of Android apps measured by their user ratings. For this, the authors selected a sample of 7,097 free Android apps from the Google Play Market and gathered information of the changes and faults that the APIs used by them presented. Using this data and statistical tools such as box-plots and the Mann-Whitney test, two main hypotheses were analyzed. The first hypothesis tested the relationship between fault-proneness (number of bugs fixed in the API) and the success of an app. The second tested the relationship between change-proneness (overall method changes, changes in method signatures and changes to the set of exceptions thrown by methods) and the success of an app. Finally, although no causal relationships between the variables can be assumed, the paper found significant differences of the level of success of the apps taking into consideration the change and fault-proneness of the APIs they use. 5.2 The Impact of API Change and Fault-Proneness on the User Ratings of Android Apps G. Bavota, M. Linares-Vásquez, C. E. Bernal-Cárdenas, M. D. Penta, R. Oliveto and D. Poshyvanyk, in IEEE Transactions on Software Engineering, vol. 41, no. 4, pp. 384-407, 1 April 2015. doi: 10.1109/TSE.2014.2367027 The paper by Bavota et al. aims to find empirical evidence supporting the success of apps and the relationship with change- and fault-proneness of the underlying APIs, where the success of the app is measured by its user rating. They performed two case studies to find quantitative evidence using 5848 free Android apps as well as an explanation for these results doing a survey with 45 professional Android developers. The quantitative case study was done by comparing the user ratings to the number of bug fixes and changes in the API that an app uses. They found that apps with a high user rating are significantly less change- and fault-prone than APIs used by apps with a low user rating. In the second case study the paper found that most of the 45 developers observed a direct relationship between the user ratings of apps and the APIs those apps use. 5.3 How can i improve my app? Classifying user reviews for software maintenance and evolution S. Panichella, A. D. Sorbo, E. Guzman, C. A. Visaggio, G. Canfora, and H. C. Gall, in 2015 ieee international conference on software maintenance and evolution (icsme), 2015, pp. 281–290. The most popular apps in the app stores (such as Google Play or App Store) receive thousands of user reviews per day and therefore it would be very time demanding to go through the reviews manually to obtain relevant information for the future development of the apps. This paper uses a combination of Natural Language Processing Sentiment Analysis and Text Analysis to extract relevant sentences from the reviews and to classify them into the following categories: Information Seeking, Information Giving, Feature Request, Problem Discovery, and Others. The results show 75% precision and 74% recall when classifier (J48 using data from NLP+SA+TA) is trained on 20% of the data (1421 manually labeled sentences from reviews of seven different apps) and the rest is used for testing. The paper also states that the results do not differ in a statistically significant manner when a different classifier is used and shows that precision and recall can be further improved by increasing the size of the data set. "],
["build-analytics.html", "Chapter 6 Build analytics 6.1 Background 6.2 Research Questions 6.3 Search Strategy 6.4 Study Selection 6.5 Summary of papers", " Chapter 6 Build analytics 6.1 Background When building a project from source code to executables everything should go smoothly. This is not always the case, a build can break for several reasons. This chapter will give an overview of research done on build scripts and continuous integration. 6.2 Research Questions What are causes of a broken build? With which goals is continuous integration applied? What can be used to effectively fix a broken build? 6.3 Search Strategy Using the initial seed consisting of Bird and Zimmermann (2017), Beller, Gousios, and Zaidman (2017a), Rausch et al. (2017), Beller, Gousios, and Zaidman (2017b), Pinto and Rebouças (2018), Zhao et al. (2017), Widder et al. (2018) and Hilton et al. (2016) we used references to find new papers to analyze. 6.4 Study Selection Through this we found the following papers 6.5 Summary of papers 6.5.1 Bird and Zimmermann (2017) Initial Seed This is a US patent grant for a method of predicting software build errors. This patent is owned by Microsoft. Using logistic regression a prediction can be made on the probability of a build failing. Using this method build errors can be better anticipated, which decreases the time until the build works again. 6.5.2 Beller, Gousios, and Zaidman (2017a) Initial Seed This paper explores data from Travis CI1 on a large scale by analyzing 2,640,825 build logs of Java and Ruby builds. It uses TravisTorrent as a data source. It is found that the number one reason for failing builds it test failure. It also explores differences in testing between Java and Ruby. 6.5.3 Rausch et al. (2017) Initial Seed A stuy on the build results of 14 open source software Java projects. It is similar to Beller, Gousios, and Zaidman (2017a), albeit on a smaller scale. It does go more in depth on the result and changes over time. 6.5.4 Beller, Gousios, and Zaidman (2017b) Initial Seed This paper introduces TravisTorrent, a dataset containing analyzed builds from more than 1,000 projects. This data is freely downloadable from the internet. It uses GHTorrent to link the information from travis to commits on GitHub. 6.5.5 Pinto and Rebouças (2018) Initial Seed This paper is a survey amongst Travis CI users. It found that users are not sure whether a job failure represents a failure or not, that inadequate testing is the most common (technical) reason for build breakage and that people feel that there is a false sense of confidence when blindly trusing tests. 6.5.6 Zhao et al. (2017) Initial Seed This paper analyzed approximately 160,000 projects written in seven different programming languages. It notes that adoption of CI is often part of a reorganization. It collected information on the differences before and after adoption of CI. There is also a survey amongst developers to learn about their experiences in adopting Travis CI. 6.5.7 Widder et al. (2018) Initial Seed This paper analyzes what factors have impact on abandonment of Travis. They find that increased build complexity reduces the chance of abandonment, but larger projects abandon at a higher rate and that a project’s language has significant but varying effect. A surprising result is that metrics of configuration attempts and knowledge dispersion in the project don’t affect the rate of abandonment. 6.5.8 Hilton et al. (2016) Initial Seed This paper explores which CI system developers use, how developers use CI and why developers use CI. For this it analyzes data from Github, Travis CI and it conducts a developer survey. It finds that projects using CI release twice as often, accept pull requests faster and have developers who are less worried about breaking the build. 6.5.9 Vassallo et al. (2017) References Beller, Gousios, and Zaidman (2017a) This paper discusses the difference in failures on continuous integration between open source software (OSS) and industrial software projects. For this 349 Java OSS projects and 418 project from ING Nederland, a financial organization. Using cluser analysis it was observed that both kinds of projects share similar build failures, but in other cases very different patterns emerge. 6.5.10 Hassan and Wang (2018) References Beller, Gousios, and Zaidman (2017b) This paper uses TravisTorrent (Beller, Gousios, and Zaidman (2017b)) to show that 22% of code commits include changes in build script files to keep the build working or to fix the build. In the paper a tool is proposed to automatically fix build failures based on previous changes. 6.5.11 Vassallo et al. (2018) References Beller, Gousios, and Zaidman (2017a), Rausch et al. (2017) This paper proposes a tool called BART to help developers fix build errors. This tool eliminates the need to browse error logs which can be very long by generating a summary of the failure with useful information. 6.5.12 Zampetti et al. (2017) Referenced by Vassallo et al. (2018) This paper studies the usage of static analysis tools in 20 Java open source software projects hosted on GitHub and using Travic CI as continuous integration infrastructure. There is investigated which tools are being used, what types of issues make the build fail or raise warnings and how is responded to broken builds. References "],
["release-engineering-analytics.html", "Chapter 7 Release Engineering Analytics 7.1 Search Strategy", " Chapter 7 Release Engineering Analytics 7.1 Search Strategy Release engineering is a relatively new research topic, given that modern processes for releasing software (e.g. continuous delivery) are industry-driven. Therefore, we took an exploratory approach in collecting any literature revolving around the topic of release engineering from the perspective of software analytics. This will aid us in determining a more narrow scope for our survey, subsequently allowing us to find additional literature fitting this scope. At the start of this project, five papers were given to us as a starting point for the literature survey. These initial papers were Adams and McIntosh (2016), Costa et al. (2016), Costa et al. (2014), Khomh et al. (2012), and Khomh et al. (2015). We collected publications using two search engines: Scopus and Google Scholar. These each encompass various databases such as ACM Digital Library, Springer, IEEE Xplore and ScienceDirect. The queries we entered are summarized in Figure 1. The publications found using this query were: Kaur and Vig (2019) Kerzazi and Robillard (2013) Castelluccio, An, and Khomh (2017) Karvonen et al. (2017) Claes et al. (2017) Fujibayashi et al. (2017) Souza, Chavez, and Bittencourt (2015) Laukkanen et al. (2018) TITLE-ABS-KEY( ( &quot;continuous release&quot; OR &quot;rapid release&quot; OR &quot;frequent release&quot; OR &quot;quick release&quot; OR &quot;speedy release&quot; OR &quot;accelerated release&quot; OR &quot;agile release&quot; OR &quot;short release&quot; OR &quot;shorter release&quot; OR &quot;lightning release&quot; OR &quot;brisk release&quot; OR &quot;hasty release&quot; OR &quot;compressed release&quot; OR &quot;release length&quot; OR &quot;release size&quot; OR &quot;release cadence&quot; OR &quot;release frequency&quot; OR &quot;continuous delivery&quot; OR &quot;rapid delivery&quot; OR &quot;frequent delivery&quot; OR &quot;fast delivery&quot; OR &quot;quick delivery&quot; OR &quot;speedy delivery&quot; OR &quot;accelerated delivery&quot; OR &quot;agile delivery&quot; OR &quot;short delivery&quot; OR &quot;lightning delivery&quot; OR &quot;brisk delivery&quot; OR &quot;hasty delivery&quot; OR &quot;compressed delivery&quot; OR &quot;delivery length&quot; OR &quot;delivery size&quot; OR &quot;delivery cadence&quot; OR &quot;continuous deployment&quot; OR &quot;rapid deployment&quot; OR &quot;frequent deployment&quot; OR &quot;fast deployment&quot; OR &quot;quick deployment&quot; OR &quot;speedy deployment&quot; OR &quot;accelerated deployment&quot; OR &quot;agile deployment&quot; OR &quot;short deployment&quot; OR &quot;lightning deployment&quot; OR &quot;brisk deployment&quot; OR &quot;hasty deployment&quot; OR &quot;compressed deployment&quot; OR &quot;deployment length&quot; OR &quot;deployment size&quot; OR &quot;deployment cadence&quot; ) AND ( &quot;release schedule&quot; OR &quot;release management&quot; OR &quot;release engineering&quot; OR &quot;release cycle&quot; OR &quot;release pipeline&quot; OR &quot;release process&quot; OR &quot;release model&quot; OR &quot;release strategy&quot; OR &quot;release strategies&quot; OR &quot;release infrastructure&quot; ) AND software ) AND ( LIMIT-TO(SUBJAREA, &quot;COMP&quot;) OR LIMIT-TO(SUBJAREA, &quot;ENGI&quot;) ) AND PUBYEAR AFT 2014 Figure 1. Query used for retrieving release engineering publications via Scopus. In addition to the search strategy that is based on a combination of keywords and subject headings as described above, a review of the list of publications that retrieved papers cite and are cited by is done. These lists are provided by Google Scholar, as well as the reference lists of the papers themselves. Results of which are listed in Table 1. Table 1. Papers found indirectly by investigating citations of/by other papers. Starting point Type Result Souza, Chavez, and Bittencourt (2015) has cited Plewnia, Dyck, and Lichter (2014) Mäntylä et al. (2015) Khomh et al. (2015) is cited by Poo-Caamaño (2016) Teixeira (2017) Mäntylä et al. (2015) is cited by Rodríguez et al. (2017) Cesar Brandão Gomes da Silva et al. (2017) In order to aggregate our collective efforts, selected sources were stored in a custom built web-based tool for conducting literature reviews. The source code of this tool is published in a GitHub repository. By commenting on and tagging our findings we were able to export a filtered list of publications, the relevance of which was agreed upon. "]
]
