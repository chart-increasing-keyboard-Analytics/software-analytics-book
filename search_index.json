[
["index.html", "A Literature Survey of Software Analytics Chapter 1 Preamble 1.1 License", " A Literature Survey of Software Analytics IN4334 2018 TU Delft 2018-09-26 Chapter 1 Preamble The book you see in front of you is the outcome of an eight week seminar run by the Software Engineering Research Group (SERG) at TU Delft. We have split up the novel area of Software Analytics into several sub topics. Every chapter addresses one such sub-topic of Software Analytics and is the outcome of a systematic literature review a laborious team of 3-4 students performed. With this book, we hope to structure the new field of Software Analytics and show how it is related to many long existing research fields. The IN4334 – Software Analytics class of 2018 1.1 License This book is copyrighted 2018 by TU Delft and its respective authors and distributed under the CC BY-NC-SA 4.0 license "],
["a-contemporary-view-on-software-analytics.html", "Chapter 2 A contemporary view on Software Analytics 2.1 What is Software Analytics? 2.2 A list of Software Analytics Sub-Topics", " Chapter 2 A contemporary view on Software Analytics 2.1 What is Software Analytics? 2.2 A list of Software Analytics Sub-Topics "],
["testing-analytics.html", "Chapter 3 Testing Analytics 3.1 Motivation 3.2 Research protocol 3.3 Extracted paper information", " Chapter 3 Testing Analytics 3.1 Motivation Testing is an important aspect in software engineering, as it forms the first line of defence against the introduction of software faults. However, in practice it seems that not all developers test actively. In this paper we will survey on the use of testing and the tools that make this possible. We will also look into the future development of tools that is done or required in order to improve testing practices in real-world applications. The above example could have been prevented by making tests but is not guaranteed to do so. Testing is not the holy grail for completely removing all bugs from a program but it can decrease the chances for a user to encounter a bug. We believe that extra research is needed to ease the life of developers by making testing more efficient, easier to maintain and more effective. Therefore, we wanted to write a survey on the testing behavior, current practices and future developments of testing. In order to perform our survey, we formulated three Research Questions (RQs): RQ1 How do developers currently test? RQ2 What state of the art technologies are being used? RQ3 What future developments can be expected? In this paper we will first elaborate on the research protocol that was used in order to find papers and extract information for the survey. Second, the actual findings for each of the research questions will be explained. 3.2 Research protocol For this paper, Kitchenham’s survey method was applied. For this method, a protocol has to be specified. This protocol is defined for the research questions given above. Below the inclusion and exclusion criteria are given, which helped finding the rightful papers. After these criteria, the actual search for papers is described. The papers that were found are listed and after they are tested against the criteria that are given. The data that is extracted from these papers are list afterward. Some papers that were left out will be listed and the reasons for leaving them out will be given to make clear why some papers do not meet the required desire. Each of the papers found was tested using our inclusion and exclusion criteria. These criteria were introduced to make sure the papers have the information required to answser the RQs while also being relevant with respect to their quality and age. Below a list of inclusion and exclusion criteria is given. In general, for all criteria, the exclusion criteria take precedence over inclusion criteria. The following inclusion and exclusion criteria were used: Papers published before 2008 are excluded from the research, unless a reference/citation is used for an unchanged concept. Papers referring to less than 15 other papers, excluding self-references, are excluded from the research. Selected papers should have an abstract, introduction and conclusion section. Papers stating the developers’ testing behavior are included. Papers stating the developers’ problems related to testing are included. Papers stating the technologies, related to testing analytics, which developers use are included. Papers writing about the expected advantage of current findings in testing analytics are included. Papers with recommendations for future development in the software testing field are included. The papers used in this paper were found by using a given initial seed of papers (query defined below as ‘Initial Paper Seed’). From this initial seed of papers we used the keywords used by those papers to construct queries, additionally the references (‘referenced by’) and the citations (‘cited in’) of the papers were used to find papers. The query row of the tables describing the references, as found below, indicates how a paper was found. For queries the default search sites were Scopus, Google Scholar and Springer. The keywords used to find papers were: software, test*, analytics, test-suite, evolution, software development, computer science, software engineering, risk-driven, survey software testing The table below describes for each paper, which Query resulted in which paper being found. Category Reference Query Test evolution Mirzaaghaei, Pastore, and Pezze (2012) Google Scholar query: test-suite evolution Test evolution Pinto, Sinha, and Orso (2013) Referenced by: Understanding myths and realities of test-suite evolution Test evolution Bevan et al. (2005) Referenced by: Understanding myths and realities of test-suite evolution Test evolution Pinto, Sinha, and Orso (2012) Initial Paper Seed Co-evolution Marsavina, Romano, and Zaidman (2014) Google Scholar keywords: Maintain developer tests, in ‘cited by’ of “Aiding Software Developers to Maintain Developer Tests” on IEEE Co-evolution Zaidman et al. (2011) Initial Paper Seed Co-evolution Greiler et al. (2013) In ‘cited by’ of “Understanding myths and realities of test-suite evolution” on Scopus Co-evolution Hurdugaci and Zaidman (2012) Keywords: Maintain developer tests, ‘cited by’ in “Studying the co-evolution of production and test code in open source and industrial developer test processes through repository mining” on IEEE Production evolution Eick et al. (2001) Referenced by: Testing analytics on software variability Production evolution @leung2015testing Initial Paper Seed Test generation Robinson et al. (2011) Referenced in Supporting Test Suite Evolution through Test Case Adaptation Test generation Bowring and Hegler (2014) Springer: Reverse search on “Automatically generating maintainable regression unit tests for programs” Test generation Shamshiri et al. (2018) Google Scholar query: Automatically generating unit tests Test generation @dulz2013model Scopus query: “software development” AND Computer Science AND Software Engineering Testing practices Garousi and Zhi (2013) Google Scholar query: Survey software testing Testing practices Beller et al. (2017) Initial Paper Seed Testing practices Beller et al. (2015) In ‘cited by’ of “Understanding myths and realities of test-suite evolution”. Testing practices Moiz (2017) Springer query: software testing Risk-driven testing Hemmati and Sharifi (2018) In ‘cited by’ of “Test case analytics: Mining test case traces to improve risk-driven testing” Risk-driven testing Schneidewind (2007) Scopus query: risk-driven testing Risk-driven testing Vernotte et al. (2015) Scopus query: “risk-driven” AND testing Risk-driven testing Atifi, Mamouni, and Marzak (2017) In ‘cited by’ of “Risk-driven software testing and reliability” Risk-driven testing Noor and Hemmati (2015) Initial Paper Seed 3.2.1 Papers per research question In this section, each of the papers is categorized with a corresponding research question. In the table above, the categories per paper were added based on their general topic. These broad topics will be assigned to a corresponding research question. All papers per research question are ordered on their relevance, which in most cases means that a newer paper is considered as more relevant than an older paper. A lower ranking may also be caused by a lower quality of writing (e.g. Greiler et al. (2013) in RQ2). The categorizations are based on the bullet points extracted from each paper. These bullet points can be found below in section ‘Extracted paper information’ below. RQ1 (How do developers currently test?): Beller et al. (2017) Beller et al. (2015) Marsavina, Romano, and Zaidman (2014) Pinto, Sinha, and Orso (2013) Garousi and Zhi (2013) Pinto, Sinha, and Orso (2012) Zaidman et al. (2011) RQ2 (What state of the art technologies are being used?): Mirzaaghaei, Pastore, and Pezze (2012) Vernotte et al. (2015) Bowring and Hegler (2014) Hurdugaci and Zaidman (2012) Robinson et al. (2011) Greiler et al. (2013) Dulz (2013) Atifi, Mamouni, and Marzak (2017) Noor and Hemmati (2015) RQ3 (What future developments can be expect?): Hemmati and Sharifi (2018) Shamshiri et al. (2018) Vernotte et al. (2015) Noor and Hemmati (2015) Mirzaaghaei, Pastore, and Pezze (2012) Bowring and Hegler (2014) Leung and Lui (2015) Greiler et al. (2013) Atifi, Mamouni, and Marzak (2017) 3.3 Extracted paper information The papers retrieved using the research protocol are reviewed for their quality and useful information is extracted to be able to answer the research questions. This information can be found in this section as a list of bullet-points. If a paper is perceived as ‘bad’ or irrelevant for answering the research questions, this is elaborated. 3.3.1 Test evolution Supporting Test Suite Evolution through Test Case Adaptation (Mirzaaghaei, Pastore, and Pezze (2012)) Test case evolution. Automatic test repairing using information available in existing test cases. Identifies a set of common actions for adapting test cases by developers. Properly repairs 90% of the compilation errors addressed and covers the same amount of instructions. Not all prototypes were tested. Claims that many test cases designed for the early versions of the system become obsolete during the software lifecycle. An approach is proposed for automatically repairing and generating test cases during software evolution. This approach uses information available in existing test cases, defines a set of heuristics to repair test cases invalidated by changes in the software, and generate new test cases for evolved software. The results show that the approach can effectively maintain evolving test suites, and perform well compared to competing approaches. Frequent actions for adapting test cases that developers commonly adopt to repair and generate test cases are identified. In general: automated test case evolution seems fairly possible. (in 2012) TestEvol: A tool for analyzing test-suite evolution (Pinto, Sinha, and Orso (2013)) Test case evolution. Tool for systematic investigating the evolution of the test-suite. Motivation: understand test maintenance in general. Only for Java and JUnit. Facilitating software evolution research with kenyon (Bevan et al. (2005)) This paper is too old based on our exclusion criteria. Understanding myths and realities of test-suite evolution (Pinto, Sinha, and Orso (2012)) Systematic measurement of how test-suites evolve Test repairs occur in practice. avg 16 repairs per version –&gt; often enough to warrant the development of automated techniques. Test repairs are not the primary reason for test modification. Non-test repair related modifications occur about 4 times as frequently. Only 10% of the tests consider fixed assert tests (oracle tests) Test repairs frequently consider repairs to method call chains. Test deletions and additions are often due to refactoring A considerable portion of the additions is due to augmenting tests to make it more *adequate. General: automated techniques may be useful. 3.3.2 Co-Evolution Studying Fine-Grained Co-evolution Patterns of Production and Test Code (Marsavina, Romano, and Zaidman (2014)) Co-evolution of production and test code. Generally co-evolving test and production code is a difficult tass. Mines fine-grained changes from the evolution of 5 open-source systems. Also uses an association rule mining algorithm to generate co-evolution patterns. The patterns are interpreted by performing a qualitative analysis. Meant to gain a deeper understanding of the way in which tests evolve as a result of changes in the production classes and identify possible gaps to signal developers for missed production code parts that have not been addressed adequately by tests. Some patterns that were found: Tests are mostly removed when production classes they cover are deleted. Programmers are careful not to leave non-compiling tests. Only limited effort is done on updating test cases after production classes are modified; tests are rarely changed when attributes or methods are changed in the production classes. A pattern indicates that mostly when numerous condition related changes are made in the production methods, test cases are created/deleted in order to address the branches that were removed/added. Test cases are rarely updated when changes related to attributes or methods are made in the production code. Test methods are in several cases created/deleted when conditional statements are altered in the production code. Future work should include the co-evolution patterns of different coding methodologies, for example, Test-Driven Development and their possible respective differences. Future work should include intent-preserving techniques, which could help ensure test repairs address the same production code functionalities as before the tests were broken. Studying the co-evolution of production and test code (Zaidman et al. (2011)) Testing is phased and co-evolution is synchronous No increase in testing activity before major releases. Intense phases were detected. Evidence for TDD discovered in 2/6 test cases. The fraction of test code (wrt prod code) increases as coverage increases Strategies for avoiding text fixture smells during software evolution (Greiler et al. (2013)) Knowledge about how and when smells in test fixtures are produced. Test fixture smells do not continuously develop over time. A correlation between the number of tests and smells. Few test cases contribute to the majority of the smells. Not the highest quality paper, the title even contains a typo, where ‘text’ should be ‘test’. Aiding Software Developers to Maintain Developer Tests (Hurdugaci and Zaidman (2012)) Support for co-evolution of testing code with production code. Introduces TestNForce (Visual Studio only), a tool to help developers to identify unit tests that need to be altered and executed after code change. Gives a broad explanation for the need for the co-evolution of test code. Three scenarios: show covering tests, enforcing self-contained commits and what tests need to run? Used an experimental setup with only eight participants from the Delft University of Technology of which two participants did not use Unit testing. Hard to generalize. On average, the participants considered 80 code coverage as “good”. 3.3.3 Production evolution Does code decay? Assessing the evidence from change management data (Eick et al. (2001)) This paper is too old based on our exclusion criteria. Testing analytics on software variability (Leung and Lui (2015)) Variability-aware testing. System integration testing has to be manually executed to evaluate the system’s compliance with its specified requirement and performance. Aids testers and developers to reduce their product time-to-market by utilizing historical testing results and similarity among systems. 3.3.4 Test generation Scaling up automated test generation: Automatically generating maintainable regression unit tests for programs (Robinson et al. (2011)) A system that has good coverage and mutation kill score, made readable code and required few edits as the system under test evolved. (stable) Statement: The costs of unit tests are not perceived to outweigh the benefits. Previous techniques: hard to understand / maintain / brittle and only tested on libraries → not real software development code. They claim they made a pretty well working test-generation tool. Obsidian: Pattern-Based Unit Test Implementations (Bowring and Hegler (2014)) A tool that generates the templates for tests: guarantee compilation, support exception handling, find suitable location… etc. Developers still need to fix the oracle tests, but the implementation/template is there. Looks at the context in order to decide what template to use. Distinguishes implementations from test cases How Do Automatically Generated Unit Tests Influence Software Maintenance? (Shamshiri et al. (2018)) Automatically generated tests are usually not based on realistic scenarios, and are therefore generally considered to be less readable. Every time a test fails, a developer has to decide whether this failure has revealed a regression fault in the program under test, or whether the test itself needs to be updated. Whilst maintenance activities take longer when working with automatically generated tests, they found developers to be equally effective with manually written and automatically generated tests. There is a need for research into the generation of more realistic tests. Model-Based Strategies for Reducing the Complexity of Statistically Generated Test Suites (Dulz (2013)) By directed adjusting specific probability values in the usage profile of a Markov chain usage model it is relatively easy to generate abstract test suites for different user classes and test purposes in an automated approach. By using proper tools, like the TestUS Testplayer even less experienced test engineers will be able to efficiently generate abstract test cases and to graphically assess quality characteristics of different test suites. 3.3.5 Testing Practices A survey of software testing practices in Canada (Garousi and Zhi (2013)) The importance of testing-related training is increasing Functional and unit-testing receive the most effort and attention The mutation testing approach is getting attention amongst Canadian firms Test last approach is still dominant, few companies try TDD In terms of popularity: NUnit and Web application testing overtook JUnit and IBM Rational tools Coverage metrics, to most commonly used: branch and conditional coverage Number of passing test / defects per day is used as the most popular metric in order to determine a release Ratio of testers : developers is somewhere around 1:2 and 1:5. The total effort is estimated to be less than 40% More than 70% of the respondents participated in a forum related to testing on a regular basis In general: more attention to testing (in 2012) Developer testing in the IDE: Patterns, beliefs and behavior (Beller et al. (2017)) Java C# developer testing behavior Little support for TDD Developers execute tests phased Only half of the developers practice testing actively Testing time is overestimated twofold. 12% of the test cases show flaky behavior Correlation between test flakiness and CI error-proneness? Few (25%) tests detect 75% of the execution failures. Tests and production code do not co-evolve gracefully. When, how, and why developers (do not) test in their IDEs (Beller et al. (2015)) Developers largely do not run tests in the IDE. However, when they do, they do it heftily. Tests run in the IDE take a short amount of time Developers run selective tests (often 1) Most test executions fail A typical reaction is to dive into offending code TDD is not widely practiced, even by those who say they do (strict definition) The way people test is different from how they believe they test. Uncertainty in Software Testing (Moiz (2017)) Mechanisms are needed to address uncertainty in each of the deliverables produced during software development process. The uncertainty metrics can help in assessing the degree of uncertainty. 3.3.6 Risk-driven testing Investigating NLP-Based Approaches for Predicting Manual Test Case Failure (Hemmati and Sharifi (2018)) System-level manual acceptance testing is one of the most expensive testing activities. A new test case failure prediction approach is proposed, which does not rely on source code or specification of the software under test. The approach uses basic Information Retrieval (IR) methods on the test case descriptions, written in natural language, based on the frequency of terms in the manual test scripts. The test fail prediction is accurate and the NLP-based feature can improve the prediction models. “To the best of our knowledge, this work is the first use of NLP on manual test case scripts for test failure prediction and has shown promising results, which we are planning to replicate on different systems and expand on different NLP-based features to more accurately extract features keywords from test cases.” Risk-driven software testing and reliability (Schneidewind (2007)) This paper is discarded from the survey, because it uses weak models to validate the claims made and is too old based on our exclusion criteria. Risk-driven vulnerability testing: Results from eHealth experiments using patterns and model-based approach (Vernotte et al. (2015)) This paper introduces and reports on an original tooled risk-driven security testing process called Pattern-driven and Model-based Vulnerability Testing. This fully automated testing process, drawing on risk-driven strategies and Model-Based Testing (MBT) techniques, aims to improve the capability of detection of various Web application vulnerabilities, in particular SQL injections, Cross-Site Scripting, and Cross-Site Request Forgery. An empirical evaluation, conducted on a complex and freely-accessible eHealth system developed by Info World, shows that this novel process is appropriate for automatically generating and executing risk-driven vulnerability test cases and is promising to be deployed for large-scale Web applications. A comparative study of software testing techniques (Atifi, Mamouni, and Marzak (2017)) They highlight two software testing techniques considered among the most used techniques to perform software tests, and then perform a comparative study of these techniques, the approaches that support studied techniques, and the tools used for each technique. The first technique is Model-based-testing, the second Risk-based testing. Test case analytics: Mining test case traces to improve risk-driven testing (Noor and Hemmati (2015)) In risk-driven testing, test cases are generated and/or prioritized based on different risk measures. *The most basic risk measure would analyze the history of the software and assigns higher risk to the test cases that used to detect bugs in the past. A new risk measure is defined which assigns a risk factor to a test case, if it is similar to a failing test case from history. *The new risk measure is by far more effective in identifying failing test cases compared to the traditional risk measure. “Though our initial and simple implementation in this paper was very promising, we are planning to investigate other similarity functions, specifically those that account for the method orders in the trace. In addition, this project is a sub-project of a bigger research on risk-driven model-based testing, where we are planning to extract specification models of the system and augment them with the similarity-based risk measures. Those models can later be used in both risk-driven test generation and prioritization.” References "],
["build-analytics.html", "Chapter 4 Build analytics 4.1 Motivation 4.2 Research Questions 4.3 Research protocol 4.4 Answers 4.5 Summary of papers", " Chapter 4 Build analytics 4.1 Motivation Ideally, when building a project from source code to executable, the process should be fast and without any errors. Unfortunately, this is not always the case and automated builds results notify developers of compile errors, missing dependencies, broken functionality and many other problems. This chapter is aimed to give an overview of the effort made in build analytics field and Continuous Integration (CI) as an increasingly common development practice in many projects. 4.2 Research Questions RQ1 What is the current state of the art in the field of build analytics? RQ2 What is the current state of practice in the field of build analytics? RQ3 What future research can we expect in the field of build analytics? 4.3 Research protocol Using the initial seed consisting of Bird and Zimmermann (2017), Moritz Beller, Gousios, and Zaidman (2017a), Rausch et al. (2017), Moritz Beller, Gousios, and Zaidman (2017b), Pinto and Rebouças (2018), Zhao et al. (2017), Widder et al. (2018) and Hilton et al. (2016) we used references to find new papers to analyze. Moreover, we used academical search engines like GoogleScholar to perform a keyword based search for other relevant build analytics domain papers. The keywords used were: build analytics, machine learning, build time, prediction, continuous integration, build failures, active learning, build errors, mining, software repositories, open-source software. 4.4 Answers Through this we found the following papers 4.5 Summary of papers 4.5.1 Bird and Zimmermann (2017) Initial Seed This is a US patent grant for a method of predicting software build errors. This patent is owned by Microsoft. Using logistic regression a prediction can be made on the probability of a build failing. Using this method build errors can be better anticipated, which decreases the time until the build works again. 4.5.2 Moritz Beller, Gousios, and Zaidman (2017a) Initial Seed This paper explores data from Travis CI1 on a large scale by analyzing 2,640,825 build logs of Java and Ruby builds. It uses TravisTorrent as a data source. It is found that the number one reason for failing builds it test failure. It also explores differences in testing between Java and Ruby. 4.5.3 Rausch et al. (2017) Initial Seed A stuy on the build results of 14 open source software Java projects. It is similar to Moritz Beller, Gousios, and Zaidman (2017a), albeit on a smaller scale. It does go more in depth on the result and changes over time. 4.5.4 Moritz Beller, Gousios, and Zaidman (2017b) Initial Seed This paper introduces TravisTorrent, a dataset containing analyzed builds from more than 1,000 projects. This data is freely downloadable from the internet. It uses GHTorrent to link the information from travis to commits on GitHub. 4.5.5 Pinto and Rebouças (2018) Initial Seed This paper is a survey amongst Travis CI users. It found that users are not sure whether a job failure represents a failure or not, that inadequate testing is the most common (technical) reason for build breakage and that people feel that there is a false sense of confidence when blindly trusing tests. 4.5.6 Zhao et al. (2017) Initial Seed This paper analyzed approximately 160,000 projects written in seven different programming languages. It notes that adoption of CI is often part of a reorganization. It collected information on the differences before and after adoption of CI. There is also a survey amongst developers to learn about their experiences in adopting Travis CI. 4.5.7 Widder et al. (2018) Initial Seed This paper analyzes what factors have impact on abandonment of Travis. They find that increased build complexity reduces the chance of abandonment, but larger projects abandon at a higher rate and that a project’s language has significant but varying effect. A surprising result is that metrics of configuration attempts and knowledge dispersion in the project do not affect the rate of abandonment. 4.5.8 Hilton et al. (2016) Initial Seed This paper explores which CI system developers use, how developers use CI and why developers use CI. For this it analyzes data from Github, Travis CI and it conducts a developer survey. It finds that projects using CI release twice as often, accept pull requests faster and have developers who are less worried about breaking the build. 4.5.9 Vassallo et al. (2017) References Moritz Beller, Gousios, and Zaidman (2017a) This paper discusses the difference in failures on continuous integration between open source software (OSS) and industrial software projects. For this 349 Java OSS projects and 418 project from ING Nederland, a financial organization. Using cluser analysis it was observed that both kinds of projects share similar build failures, but in other cases very different patterns emerge. 4.5.10 Hassan and Wang (2018) References Moritz Beller, Gousios, and Zaidman (2017b) This paper uses TravisTorrent (Moritz Beller, Gousios, and Zaidman (2017b)) to show that 22% of code commits include changes in build script files to keep the build working or to fix the build. In the paper a tool is proposed to automatically fix build failures based on previous changes. 4.5.11 Vassallo et al. (2018) References Moritz Beller, Gousios, and Zaidman (2017a), Rausch et al. (2017) This paper proposes a tool called BART to help developers fix build errors. This tool eliminates the need to browse error logs which can be very long by generating a summary of the failure with useful information. 4.5.12 Zampetti et al. (2017) Referenced by Vassallo et al. (2018) This paper studies the usage of static analysis tools in 20 Java open source software projects hosted on GitHub and using Travic CI as continuous integration infrastructure. There is investigated which tools are being used, what types of issues make the build fail or raise warnings and how is responded to broken builds. References "],
["sample-sub-topic.html", "Chapter 5 Sample Sub-Topic 5.1 Motivation 5.2 Research protocol 5.3 Answers", " Chapter 5 Sample Sub-Topic This is an example for the deliverable every group works on. Every group works on one independent chapter (starting as one Rmd file). 5.1 Motivation A short introduction about why the topic you are working on is interesting. The RQs that everyone should be aiming at are: RQ1 Current state of the art in software analytics for your topic : Topics that are being explored Research methods, tools and datasets being used Main research findings, aggregated RQ2 Current state of practice in software analytics for your topic : Tools and companies creating / employing them Case studies and their findings RQ3 Open challenges and future research required 5.2 Research protocol Here, you describe the details of applying Kitchenham’s survey method for your topic, including search queries, fact extraction, coding process and an intial groupping of the papers that you will be analyzing. 5.3 Answers Aggregated answers to the RQs, per RQ. You need: For RQ1 Topics that are being explored Research methods, tools and datasets being used Main research findings, aggregated For RQ2 : Tools and companies creating / employing them Case studies and their findings For RQ3: List of challenges An aggregated set of open research items, as described in the papers Research questions that emerge from the synthesis of the presented works "],
["sample-sub-topic-1.html", "Chapter 6 Sample Sub-Topic 6.1 Motivation 6.2 Research protocol 6.3 Answers", " Chapter 6 Sample Sub-Topic This is an example for the deliverable every group works on. Every group works on one independent chapter (starting as one Rmd file). 6.1 Motivation A short introduction about why the topic you are working on is interesting. The RQs that everyone should be aiming at are: RQ1 Current state of the art in software analytics for your topic : Topics that are being explored Research methods, tools and datasets being used Main research findings, aggregated RQ2 Current state of practice in software analytics for your topic : Tools and companies creating / employing them Case studies and their findings RQ3 Open challenges and future research required 6.2 Research protocol Here, you describe the details of applying Kitchenham’s survey method for your topic, including search queries, fact extraction, coding process and an intial groupping of the papers that you will be analyzing. 6.3 Answers Aggregated answers to the RQs, per RQ. You need: For RQ1 Topics that are being explored Research methods, tools and datasets being used Main research findings, aggregated For RQ2 : Tools and companies creating / employing them Case studies and their findings For RQ3: List of challenges An aggregated set of open research items, as described in the papers Research questions that emerge from the synthesis of the presented works "],
["release-engineering-analytics.html", "Chapter 7 Release Engineering Analytics 7.1 Search Strategy", " Chapter 7 Release Engineering Analytics 7.1 Search Strategy Release engineering is a relatively new research topic, given that modern processes for releasing software (e.g. continuous delivery) are industry-driven. Therefore, we took an exploratory approach in collecting any literature revolving around the topic of release engineering from the perspective of software analytics. This will aid us in determining a more narrow scope for our survey, subsequently allowing us to find additional literature fitting this scope. At the start of this project, five papers were given to us as a starting point for the literature survey. These initial papers were Adams and McIntosh (2016), Costa et al. (2016), Costa et al. (2014), Khomh et al. (2012), and Khomh et al. (2015). We collected publications using two search engines: Scopus and Google Scholar. These each encompass various databases such as ACM Digital Library, Springer, IEEE Xplore and ScienceDirect. The queries we entered are summarized in Figure 1. The publications found using this query were: Kaur and Vig (2019) Kerzazi and Robillard (2013) Castelluccio, An, and Khomh (2017) Karvonen et al. (2017) Claes et al. (2017) Fujibayashi et al. (2017) Souza, Chavez, and Bittencourt (2015) Laukkanen et al. (2018) TITLE-ABS-KEY( ( &quot;continuous release&quot; OR &quot;rapid release&quot; OR &quot;frequent release&quot; OR &quot;quick release&quot; OR &quot;speedy release&quot; OR &quot;accelerated release&quot; OR &quot;agile release&quot; OR &quot;short release&quot; OR &quot;shorter release&quot; OR &quot;lightning release&quot; OR &quot;brisk release&quot; OR &quot;hasty release&quot; OR &quot;compressed release&quot; OR &quot;release length&quot; OR &quot;release size&quot; OR &quot;release cadence&quot; OR &quot;release frequency&quot; OR &quot;continuous delivery&quot; OR &quot;rapid delivery&quot; OR &quot;frequent delivery&quot; OR &quot;fast delivery&quot; OR &quot;quick delivery&quot; OR &quot;speedy delivery&quot; OR &quot;accelerated delivery&quot; OR &quot;agile delivery&quot; OR &quot;short delivery&quot; OR &quot;lightning delivery&quot; OR &quot;brisk delivery&quot; OR &quot;hasty delivery&quot; OR &quot;compressed delivery&quot; OR &quot;delivery length&quot; OR &quot;delivery size&quot; OR &quot;delivery cadence&quot; OR &quot;continuous deployment&quot; OR &quot;rapid deployment&quot; OR &quot;frequent deployment&quot; OR &quot;fast deployment&quot; OR &quot;quick deployment&quot; OR &quot;speedy deployment&quot; OR &quot;accelerated deployment&quot; OR &quot;agile deployment&quot; OR &quot;short deployment&quot; OR &quot;lightning deployment&quot; OR &quot;brisk deployment&quot; OR &quot;hasty deployment&quot; OR &quot;compressed deployment&quot; OR &quot;deployment length&quot; OR &quot;deployment size&quot; OR &quot;deployment cadence&quot; ) AND ( &quot;release schedule&quot; OR &quot;release management&quot; OR &quot;release engineering&quot; OR &quot;release cycle&quot; OR &quot;release pipeline&quot; OR &quot;release process&quot; OR &quot;release model&quot; OR &quot;release strategy&quot; OR &quot;release strategies&quot; OR &quot;release infrastructure&quot; ) AND software ) AND ( LIMIT-TO(SUBJAREA, &quot;COMP&quot;) OR LIMIT-TO(SUBJAREA, &quot;ENGI&quot;) ) AND PUBYEAR AFT 2014 Figure 1. Query used for retrieving release engineering publications via Scopus. In addition to the search strategy that is based on a combination of keywords and subject headings as described above, a review of the list of publications that retrieved papers cite and are cited by is done. These lists are provided by Google Scholar, as well as the reference lists of the papers themselves. Results of which are listed in Table 1. Table 1. Papers found indirectly by investigating citations of/by other papers. Starting point Type Result Souza, Chavez, and Bittencourt (2015) has cited Plewnia, Dyck, and Lichter (2014) Mäntylä et al. (2015) Khomh et al. (2015) is cited by Poo-Caamaño (2016) Teixeira (2017) Mäntylä et al. (2015) is cited by Rodríguez et al. (2017) Cesar Brandão Gomes da Silva et al. (2017) In order to aggregate our collective efforts, selected sources were stored in a custom built web-based tool for conducting literature reviews. The source code of this tool is published in a GitHub repository. By commenting on and tagging our findings we were able to export a filtered list of publications, the relevance of which was agreed upon. References "],
["sample-sub-topic-2.html", "Chapter 8 Sample Sub-Topic 8.1 Motivation 8.2 Research protocol 8.3 Answers", " Chapter 8 Sample Sub-Topic This is an example for the deliverable every group works on. Every group works on one independent chapter (starting as one Rmd file). 8.1 Motivation A short introduction about why the topic you are working on is interesting. The RQs that everyone should be aiming at are: RQ1 Current state of the art in software analytics for your topic : Topics that are being explored Research methods, tools and datasets being used Main research findings, aggregated RQ2 Current state of practice in software analytics for your topic : Tools and companies creating / employing them Case studies and their findings RQ3 Open challenges and future research required 8.2 Research protocol Here, you describe the details of applying Kitchenham’s survey method for your topic, including search queries, fact extraction, coding process and an intial groupping of the papers that you will be analyzing. 8.3 Answers Aggregated answers to the RQs, per RQ. You need: For RQ1 Topics that are being explored Research methods, tools and datasets being used Main research findings, aggregated For RQ2 : Tools and companies creating / employing them Case studies and their findings For RQ3: List of challenges An aggregated set of open research items, as described in the papers Research questions that emerge from the synthesis of the presented works "],
["sample-sub-topic-3.html", "Chapter 9 Sample Sub-Topic 9.1 Motivation 9.2 Research protocol 9.3 Answers", " Chapter 9 Sample Sub-Topic This is an example for the deliverable every group works on. Every group works on one independent chapter (starting as one Rmd file). 9.1 Motivation A short introduction about why the topic you are working on is interesting. The RQs that everyone should be aiming at are: RQ1 Current state of the art in software analytics for your topic : Topics that are being explored Research methods, tools and datasets being used Main research findings, aggregated RQ2 Current state of practice in software analytics for your topic : Tools and companies creating / employing them Case studies and their findings RQ3 Open challenges and future research required 9.2 Research protocol Here, you describe the details of applying Kitchenham’s survey method for your topic, including search queries, fact extraction, coding process and an intial groupping of the papers that you will be analyzing. 9.3 Answers Aggregated answers to the RQs, per RQ. You need: For RQ1 Topics that are being explored Research methods, tools and datasets being used Main research findings, aggregated For RQ2 : Tools and companies creating / employing them Case studies and their findings For RQ3: List of challenges An aggregated set of open research items, as described in the papers Research questions that emerge from the synthesis of the presented works "],
["app-store-analytics.html", "Chapter 10 App Store analytics 10.1 API change and fault proneness: A threat to the success of Android apps 10.2 The Impact of API Change and Fault-Proneness on the User Ratings of Android Apps 10.3 How can i improve my app? Classifying user reviews for software maintenance and evolution", " Chapter 10 App Store analytics 10.1 API change and fault proneness: A threat to the success of Android apps M. Linares-Vásquez, G. Bavota, C. Bernal-Cárdenas, M. Di Penta, R. Oliveto, and D. Poshyvanyk, in Proceedings of the 2013 9th joint meeting on foundations of software engineering, 2013, pp. 477–487. The paper presents an empirical study that aims to corroborate the relationship between the fault and change-proneness of APIs and the degree of success of Android apps measured by their user ratings. For this, the authors selected a sample of 7,097 free Android apps from the Google Play Market and gathered information of the changes and faults that the APIs used by them presented. Using this data and statistical tools such as box-plots and the Mann-Whitney test, two main hypotheses were analyzed. The first hypothesis tested the relationship between fault-proneness (number of bugs fixed in the API) and the success of an app. The second tested the relationship between change-proneness (overall method changes, changes in method signatures and changes to the set of exceptions thrown by methods) and the success of an app. Finally, although no causal relationships between the variables can be assumed, the paper found significant differences of the level of success of the apps taking into consideration the change and fault-proneness of the APIs they use. 10.2 The Impact of API Change and Fault-Proneness on the User Ratings of Android Apps G. Bavota, M. Linares-Vásquez, C. E. Bernal-Cárdenas, M. D. Penta, R. Oliveto and D. Poshyvanyk, in IEEE Transactions on Software Engineering, vol. 41, no. 4, pp. 384-407, 1 April 2015. doi: 10.1109/TSE.2014.2367027 The paper by Bavota et al. aims to find empirical evidence supporting the success of apps and the relationship with change- and fault-proneness of the underlying APIs, where the success of the app is measured by its user rating. They performed two case studies to find quantitative evidence using 5848 free Android apps as well as an explanation for these results doing a survey with 45 professional Android developers. The quantitative case study was done by comparing the user ratings to the number of bug fixes and changes in the API that an app uses. They found that apps with a high user rating are significantly less change- and fault-prone than APIs used by apps with a low user rating. In the second case study the paper found that most of the 45 developers observed a direct relationship between the user ratings of apps and the APIs those apps use. 10.3 How can i improve my app? Classifying user reviews for software maintenance and evolution S. Panichella, A. D. Sorbo, E. Guzman, C. A. Visaggio, G. Canfora, and H. C. Gall, in 2015 ieee international conference on software maintenance and evolution (icsme), 2015, pp. 281–290. The most popular apps in the app stores (such as Google Play or App Store) receive thousands of user reviews per day and therefore it would be very time demanding to go through the reviews manually to obtain relevant information for the future development of the apps. This paper uses a combination of Natural Language Processing Sentiment Analysis and Text Analysis to extract relevant sentences from the reviews and to classify them into the following categories: Information Seeking, Information Giving, Feature Request, Problem Discovery, and Others. The results show 75% precision and 74% recall when classifier (J48 using data from NLP+SA+TA) is trained on 20% of the data (1421 manually labeled sentences from reviews of seven different apps) and the rest is used for testing. The paper also states that the results do not differ in a statistically significant manner when a different classifier is used and shows that precision and recall can be further improved by increasing the size of the data set. "],
["final-words.html", "Chapter 11 Final Words", " Chapter 11 Final Words We have finished a nice book on Software Analytics. "],
["references.html", "References", " References "]
]
