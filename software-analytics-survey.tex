\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={A Literature Survey of Software Analytics},
            pdfauthor={IN4334 2018 TU Delft},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{apalike}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{A Literature Survey of Software Analytics}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{IN4334 2018 TU Delft}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{2018-09-26}

\usepackage{booktabs}
\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\chapter{Preamble}\label{intro}

The book you see in front of you is the outcome of an eight week seminar
run by the Software Engineering Research Group (SERG) at TU Delft. We
have split up the novel area of Software Analytics into several sub
topics. Every chapter addresses one such sub-topic of Software Analytics
and is the outcome of a systematic literature review a laborious team of
3-4 students performed.

With this book, we hope to structure the new field of Software Analytics
and show how it is related to many long existing research fields.

\emph{The IN4334 -- Software Analytics class of 2018}

\section{License}\label{license}

\includegraphics{figures/cc-nc-sa.png} This book is copyrighted 2018 by
TU Delft and its respective authors and distributed under the
\href{https://creativecommons.org/licenses/by-nc-sa/4.0/}{CC BY-NC-SA
4.0 license}

\chapter{A contemporary view on Software
Analytics}\label{a-contemporary-view-on-software-analytics}

\section{What is Software Analytics?}\label{what-is-software-analytics}

\section{A list of Software Analytics
Sub-Topics}\label{a-list-of-software-analytics-sub-topics}

\chapter{Testing Analytics}\label{testing-analytics}

\section{Motivation}\label{motivation}

Testing is an important aspect in software engineering, as it forms the
first line of defence against the introduction of software
faults\cite{pinto2012understanding}. However, in practice it seems that
not all developers test actively. In this paper we will survey on the
use of testing and the tools that make this possible. We will also look
into the future development of tools that is done or required in order
to improve testing practices in real-world applications. The above
example could have been prevented by making tests but is not guaranteed
to do so. Testing is not the holy grail for completely removing all bugs
from a program but it can decrease the chances for a user to encounter a
bug. We believe that extra research is needed to ease the life of
developers by making testing more efficient, easier to maintain and more
effective. Therefore, we wanted to write a survey on the testing
behavior, current practices and future developments of testing. In order
to perform our survey, we formulated three Research Questions (RQs):

\begin{itemize}
\tightlist
\item
  \textbf{RQ1} How do developers currently test?
\item
  \textbf{RQ2} What state of the art technologies are being used?
\item
  \textbf{RQ3} What future developments can be expected? In this paper
  we will first elaborate on the research protocol that was used in
  order to find papers and extract information for the survey. Second,
  the actual findings for each of the research questions will be
  explained.
\end{itemize}

\section{Research protocol}\label{research-protocol}

For this paper, Kitchenham's survey method was applied. For this method,
a protocol has to be specified. This protocol is defined for the
research questions given above. Below the inclusion and exclusion
criteria are given, which helped finding the rightful papers. After
these criteria, the actual search for papers is described. The papers
that were found are listed and after they are tested against the
criteria that are given. The data that is extracted from these papers
are list afterward. Some papers that were left out will be listed and
the reasons for leaving them out will be given to make clear why some
papers do not meet the required desire.

Each of the papers found was tested using our inclusion and exclusion
criteria. These criteria were introduced to make sure the papers have
the information required to answser the RQs while also being relevant
with respect to their quality and age. Below a list of inclusion and
exclusion criteria is given. In general, for all criteria, the exclusion
criteria take precedence over inclusion criteria. The following
inclusion and exclusion criteria were used:

\begin{itemize}
\tightlist
\item
  Papers published before 2008 are excluded from the research, unless a
  reference/citation is used for an unchanged concept.
\item
  Papers referring to less than 15 other papers, excluding
  self-references, are excluded from the research.
\item
  Selected papers should have an abstract, introduction and conclusion
  section.
\item
  Papers stating the developers' testing behavior are included.
\item
  Papers stating the developers' problems related to testing are
  included.
\item
  Papers stating the technologies, related to testing analytics, which
  developers use are included.
\item
  Papers writing about the expected advantage of current findings in
  testing analytics are included.
\item
  Papers with recommendations for future development in the software
  testing field are included.
\end{itemize}

The papers used in this paper were found by using a given initial seed
of papers (query defined below as `Initial Paper Seed'). From this
initial seed of papers we used the keywords used by those papers to
construct queries, additionally the references (`referenced by') and the
citations (`cited in') of the papers were used to find papers. The query
row of the tables describing the references, as found below, indicates
how a paper was found. For queries the default search sites were Scopus,
Google Scholar and Springer.

The keywords used to find papers were: software, test*, analytics,
test-suite, evolution, software development, computer science, software
engineering, risk-driven, survey software testing

The table below describes for each paper, which Query resulted in which
paper being found.

\begin{longtable}[]{@{}lll@{}}
\toprule
\begin{minipage}[b]{0.19\columnwidth}\raggedright\strut
Category\strut
\end{minipage} & \begin{minipage}[b]{0.41\columnwidth}\raggedright\strut
Reference\strut
\end{minipage} & \begin{minipage}[b]{0.32\columnwidth}\raggedright\strut
Query\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.19\columnwidth}\raggedright\strut
Test evolution\strut
\end{minipage} & \begin{minipage}[t]{0.41\columnwidth}\raggedright\strut
\citet{supportingtestsuite}\strut
\end{minipage} & \begin{minipage}[t]{0.32\columnwidth}\raggedright\strut
Google Scholar query: test-suite evolution\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.19\columnwidth}\raggedright\strut
Test evolution\strut
\end{minipage} & \begin{minipage}[t]{0.41\columnwidth}\raggedright\strut
\citet{pinto2013}\strut
\end{minipage} & \begin{minipage}[t]{0.32\columnwidth}\raggedright\strut
Referenced by: Understanding myths and realities of test-suite
evolution\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.19\columnwidth}\raggedright\strut
Test evolution\strut
\end{minipage} & \begin{minipage}[t]{0.41\columnwidth}\raggedright\strut
\citet{bevan2005}\strut
\end{minipage} & \begin{minipage}[t]{0.32\columnwidth}\raggedright\strut
Referenced by: Understanding myths and realities of test-suite
evolution\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.19\columnwidth}\raggedright\strut
Test evolution\strut
\end{minipage} & \begin{minipage}[t]{0.41\columnwidth}\raggedright\strut
\citet{pinto2012understanding}\strut
\end{minipage} & \begin{minipage}[t]{0.32\columnwidth}\raggedright\strut
Initial Paper Seed\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.19\columnwidth}\raggedright\strut
Co-evolution\strut
\end{minipage} & \begin{minipage}[t]{0.41\columnwidth}\raggedright\strut
\citet{marsavina2014}\strut
\end{minipage} & \begin{minipage}[t]{0.32\columnwidth}\raggedright\strut
Google Scholar keywords: Maintain developer tests, in `cited by' of
``Aiding Software Developers to Maintain Developer Tests'' on IEEE\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.19\columnwidth}\raggedright\strut
Co-evolution\strut
\end{minipage} & \begin{minipage}[t]{0.41\columnwidth}\raggedright\strut
\citet{zaidman2011studying}\strut
\end{minipage} & \begin{minipage}[t]{0.32\columnwidth}\raggedright\strut
Initial Paper Seed\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.19\columnwidth}\raggedright\strut
Co-evolution\strut
\end{minipage} & \begin{minipage}[t]{0.41\columnwidth}\raggedright\strut
\citet{greiler2013}\strut
\end{minipage} & \begin{minipage}[t]{0.32\columnwidth}\raggedright\strut
In `cited by' of ``Understanding myths and realities of test-suite
evolution'' on Scopus\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.19\columnwidth}\raggedright\strut
Co-evolution\strut
\end{minipage} & \begin{minipage}[t]{0.41\columnwidth}\raggedright\strut
\citet{hurdugaci2012}\strut
\end{minipage} & \begin{minipage}[t]{0.32\columnwidth}\raggedright\strut
Keywords: Maintain developer tests, `cited by' in ``Studying the
co-evolution of production and test code in open source and industrial
developer test processes through repository mining'' on IEEE\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.19\columnwidth}\raggedright\strut
Production evolution\strut
\end{minipage} & \begin{minipage}[t]{0.41\columnwidth}\raggedright\strut
\citet{eick2001}\strut
\end{minipage} & \begin{minipage}[t]{0.32\columnwidth}\raggedright\strut
Referenced by: Testing analytics on software variability\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.19\columnwidth}\raggedright\strut
Production evolution\strut
\end{minipage} & \begin{minipage}[t]{0.41\columnwidth}\raggedright\strut
@leung2015testing\strut
\end{minipage} & \begin{minipage}[t]{0.32\columnwidth}\raggedright\strut
Initial Paper Seed\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.19\columnwidth}\raggedright\strut
\strut
\end{minipage} & \begin{minipage}[t]{0.41\columnwidth}\raggedright\strut
\strut
\end{minipage} & \begin{minipage}[t]{0.32\columnwidth}\raggedright\strut
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.19\columnwidth}\raggedright\strut
Test generation\strut
\end{minipage} & \begin{minipage}[t]{0.41\columnwidth}\raggedright\strut
\citet{robinson2011}\strut
\end{minipage} & \begin{minipage}[t]{0.32\columnwidth}\raggedright\strut
Referenced in Supporting Test Suite Evolution through Test Case
Adaptation\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.19\columnwidth}\raggedright\strut
Test generation\strut
\end{minipage} & \begin{minipage}[t]{0.41\columnwidth}\raggedright\strut
\citet{bowring2014obsidian}\strut
\end{minipage} & \begin{minipage}[t]{0.32\columnwidth}\raggedright\strut
Springer: Reverse search on ``Automatically generating maintainable
regression unit tests for programs''\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.19\columnwidth}\raggedright\strut
Test generation\strut
\end{minipage} & \begin{minipage}[t]{0.41\columnwidth}\raggedright\strut
\citet{shamshiri2018automatically}\strut
\end{minipage} & \begin{minipage}[t]{0.32\columnwidth}\raggedright\strut
Google Scholar query: Automatically generating unit tests\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.19\columnwidth}\raggedright\strut
Test generation\strut
\end{minipage} & \begin{minipage}[t]{0.41\columnwidth}\raggedright\strut
@dulz2013model\strut
\end{minipage} & \begin{minipage}[t]{0.32\columnwidth}\raggedright\strut
Scopus query: ``software development'' AND Computer Science AND Software
Engineering\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.19\columnwidth}\raggedright\strut
\strut
\end{minipage} & \begin{minipage}[t]{0.41\columnwidth}\raggedright\strut
\strut
\end{minipage} & \begin{minipage}[t]{0.32\columnwidth}\raggedright\strut
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.19\columnwidth}\raggedright\strut
Testing practices\strut
\end{minipage} & \begin{minipage}[t]{0.41\columnwidth}\raggedright\strut
\citet{GAROUSI20131354}\strut
\end{minipage} & \begin{minipage}[t]{0.32\columnwidth}\raggedright\strut
Google Scholar query: Survey software testing\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.19\columnwidth}\raggedright\strut
Testing practices\strut
\end{minipage} & \begin{minipage}[t]{0.41\columnwidth}\raggedright\strut
\citet{beller2017developer}\strut
\end{minipage} & \begin{minipage}[t]{0.32\columnwidth}\raggedright\strut
Initial Paper Seed\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.19\columnwidth}\raggedright\strut
Testing practices\strut
\end{minipage} & \begin{minipage}[t]{0.41\columnwidth}\raggedright\strut
\citet{beller2015}\strut
\end{minipage} & \begin{minipage}[t]{0.32\columnwidth}\raggedright\strut
In `cited by' of ``Understanding myths and realities of test-suite
evolution''.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.19\columnwidth}\raggedright\strut
Testing practices\strut
\end{minipage} & \begin{minipage}[t]{0.41\columnwidth}\raggedright\strut
\citet{moiz2017uncertainty}\strut
\end{minipage} & \begin{minipage}[t]{0.32\columnwidth}\raggedright\strut
Springer query: software testing\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.19\columnwidth}\raggedright\strut
\strut
\end{minipage} & \begin{minipage}[t]{0.41\columnwidth}\raggedright\strut
\strut
\end{minipage} & \begin{minipage}[t]{0.32\columnwidth}\raggedright\strut
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.19\columnwidth}\raggedright\strut
Risk-driven testing\strut
\end{minipage} & \begin{minipage}[t]{0.41\columnwidth}\raggedright\strut
\citet{hemmati2018}\strut
\end{minipage} & \begin{minipage}[t]{0.32\columnwidth}\raggedright\strut
In `cited by' of ``Test case analytics: Mining test case traces to
improve risk-driven testing''\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.19\columnwidth}\raggedright\strut
Risk-driven testing\strut
\end{minipage} & \begin{minipage}[t]{0.41\columnwidth}\raggedright\strut
\citet{schneidewind2007}\strut
\end{minipage} & \begin{minipage}[t]{0.32\columnwidth}\raggedright\strut
Scopus query: risk-driven testing\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.19\columnwidth}\raggedright\strut
Risk-driven testing\strut
\end{minipage} & \begin{minipage}[t]{0.41\columnwidth}\raggedright\strut
\citet{vernotte2015}\strut
\end{minipage} & \begin{minipage}[t]{0.32\columnwidth}\raggedright\strut
Scopus query: ``risk-driven'' AND testing\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.19\columnwidth}\raggedright\strut
Risk-driven testing\strut
\end{minipage} & \begin{minipage}[t]{0.41\columnwidth}\raggedright\strut
\citet{atifi2017}\strut
\end{minipage} & \begin{minipage}[t]{0.32\columnwidth}\raggedright\strut
In `cited by' of ``Risk-driven software testing and reliability''\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.19\columnwidth}\raggedright\strut
Risk-driven testing\strut
\end{minipage} & \begin{minipage}[t]{0.41\columnwidth}\raggedright\strut
\citet{noor2015test}\strut
\end{minipage} & \begin{minipage}[t]{0.32\columnwidth}\raggedright\strut
Initial Paper Seed\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\subsection{Papers per research
question}\label{papers-per-research-question}

In this section, each of the papers is categorized with a corresponding
research question. In the table above, the categories per paper were
added based on their general topic. These broad topics will be assigned
to a corresponding research question. All papers per research question
are ordered on their relevance, which in most cases means that a newer
paper is considered as more relevant than an older paper. A lower
ranking may also be caused by a lower quality of writing (e.g.
\citet{greiler2013} in RQ2). The categorizations are based on the bullet
points extracted from each paper. These bullet points can be found below
in section \emph{`Extracted paper information'} below.

\begin{itemize}
\tightlist
\item
  \textbf{RQ1} (\emph{How do developers currently test?}):

  \begin{itemize}
  \tightlist
  \item
    \citet{beller2017developer}
  \item
    \citet{beller2015}
  \item
    \citet{marsavina2014}
  \item
    \citet{pinto2013}
  \item
    \citet{GAROUSI20131354}
  \item
    \citet{pinto2012understanding}
  \item
    \citet{zaidman2011studying}
  \end{itemize}
\item
  \textbf{RQ2} (\emph{What state of the art technologies are being
  used?}):

  \begin{itemize}
  \tightlist
  \item
    \citet{supportingtestsuite}
  \item
    \citet{vernotte2015}
  \item
    \citet{bowring2014obsidian}
  \item
    \citet{hurdugaci2012}
  \item
    \citet{robinson2011}
  \item
    \citet{greiler2013}
  \item
    \citet{dulz2013model}
  \item
    \citet{atifi2017}
  \item
    \citet{noor2015test}
  \end{itemize}
\item
  \textbf{RQ3} (\emph{What future developments can be expect?}):

  \begin{itemize}
  \tightlist
  \item
    \citet{hemmati2018}
  \item
    \citet{shamshiri2018automatically}
  \item
    \citet{vernotte2015}
  \item
    \citet{noor2015test}
  \item
    \citet{supportingtestsuite}
  \item
    \citet{bowring2014obsidian}
  \item
    \citet{leung2015testing}
  \item
    \citet{greiler2013}
  \item
    \citet{atifi2017}
  \end{itemize}
\end{itemize}

\section{Extracted paper information}\label{extracted-paper-information}

The papers retrieved using the research protocol are reviewed for their
quality and useful information is extracted to be able to answer the
research questions. This information can be found in this section as a
list of bullet-points. If a paper is perceived as `bad' or irrelevant
for answering the research questions, this is elaborated.

\subsection{Test evolution}\label{test-evolution}

Supporting Test Suite Evolution through Test Case Adaptation
(\citet{supportingtestsuite})

\begin{itemize}
\tightlist
\item
  Test case evolution.
\item
  Automatic test repairing using information available in existing test
  cases.
\item
  Identifies a set of common actions for adapting test cases by
  developers.
\item
  Properly repairs 90\% of the compilation errors addressed and covers
  the same amount of instructions.
\item
  Not all prototypes were tested.
\item
  Claims that many test cases designed for the early versions of the
  system become obsolete during the software lifecycle.
\item
  An approach is proposed for automatically repairing and generating
  test cases during software evolution.
\item
  This approach uses information available in existing test cases,
  defines a set of heuristics to repair test cases invalidated by
  changes in the software, and generate new test cases for evolved
  software.
\item
  The results show that the approach can effectively maintain evolving
  test suites, and perform well compared to competing approaches.
\item
  Frequent actions for adapting test cases that developers commonly
  adopt to repair and generate test cases are identified.
\item
  In general: automated test case evolution seems fairly possible. (in
  2012)
\end{itemize}

TestEvol: A tool for analyzing test-suite evolution (\citet{pinto2013})

\begin{itemize}
\tightlist
\item
  Test case evolution.
\item
  Tool for systematic investigating the evolution of the test-suite.
\item
  Motivation: understand test maintenance in general.
\item
  Only for Java and JUnit.
\end{itemize}

Facilitating software evolution research with kenyon (\citet{bevan2005})
This paper is too old based on our exclusion criteria.

Understanding myths and realities of test-suite evolution
(\citet{pinto2012understanding})

\begin{itemize}
\tightlist
\item
  Systematic measurement of how test-suites evolve
\item
  Test repairs occur in practice. avg 16 repairs per version
  --\textgreater{} often enough to warrant the development of automated
  techniques.
\item
  Test repairs are not the primary reason for test modification.
  Non-test repair related modifications occur about 4 times as
  frequently.
\item
  Only 10\% of the tests consider fixed assert tests (oracle tests)
\item
  Test repairs frequently consider repairs to method call chains.
\item
  Test deletions and additions are often due to refactoring
\item
  A considerable portion of the additions is due to augmenting tests to
  make it more *adequate.
\item
  General: automated techniques may be useful.
\end{itemize}

\subsection{Co-Evolution}\label{co-evolution}

Studying Fine-Grained Co-evolution Patterns of Production and Test Code
(\citet{marsavina2014})

\begin{itemize}
\tightlist
\item
  Co-evolution of production and test code.
\item
  Generally co-evolving test and production code is a difficult tass.
\item
  Mines fine-grained changes from the evolution of 5 open-source
  systems.
\item
  Also uses an association rule mining algorithm to generate
  co-evolution patterns.
\item
  The patterns are interpreted by performing a qualitative analysis.
\item
  Meant to gain a deeper understanding of the way in which tests evolve
  as a result of changes in the production classes and identify possible
  gaps to signal developers for missed production code parts that have
  not been addressed adequately by tests.
\item
  Some patterns that were found:
\item
  Tests are mostly removed when production classes they cover are
  deleted. Programmers are careful not to leave non-compiling tests.
\item
  Only limited effort is done on updating test cases after production
  classes are modified; tests are rarely changed when attributes or
  methods are changed in the production classes.
\item
  A pattern indicates that mostly when numerous condition related
  changes are made in the production methods, test cases are
  created/deleted in order to address the branches that were
  removed/added.
\item
  Test cases are rarely updated when changes related to attributes or
  methods are made in the production code.
\item
  Test methods are in several cases created/deleted when conditional
  statements are altered in the production code.
\item
  Future work should include the co-evolution patterns of different
  coding methodologies, for example, Test-Driven Development and their
  possible respective differences.
\item
  Future work should include intent-preserving techniques, which could
  help ensure test repairs address the same production code
  functionalities as before the tests were broken.
\end{itemize}

Studying the co-evolution of production and test code
(\citet{zaidman2011studying})

\begin{itemize}
\tightlist
\item
  Testing is phased and co-evolution is synchronous
\item
  No increase in testing activity before major releases. Intense phases
  were detected.
\item
  Evidence for TDD discovered in 2/6 test cases.
\item
  The fraction of test code (wrt prod code) increases as coverage
  increases
\end{itemize}

Strategies for avoiding text fixture smells during software evolution
(\citet{greiler2013})

\begin{itemize}
\tightlist
\item
  Knowledge about how and when smells in test fixtures are produced.
\item
  Test fixture smells do not continuously develop over time.
\item
  A correlation between the number of tests and smells.
\item
  Few test cases contribute to the majority of the smells.
\item
  Not the highest quality paper, the title even contains a typo, where
  `text' should be `test'.
\end{itemize}

Aiding Software Developers to Maintain Developer Tests
(\citet{hurdugaci2012})

\begin{itemize}
\tightlist
\item
  Support for co-evolution of testing code with production code.
\item
  Introduces TestNForce (Visual Studio only), a tool to help developers
  to identify unit tests that need to be altered and executed after code
  change.
\item
  Gives a broad explanation for the need for the co-evolution of test
  code.
\item
  Three scenarios: show covering tests, enforcing self-contained commits
  and what tests need to run?
\item
  Used an experimental setup with only eight participants from the Delft
  University of Technology of which two participants did not use Unit
  testing. Hard to generalize.
\item
  On average, the participants considered 80 code coverage as ``good''.
\end{itemize}

\subsection{Production evolution}\label{production-evolution}

Does code decay? Assessing the evidence from change management data
(\citet{eick2001}) This paper is too old based on our exclusion
criteria.

Testing analytics on software variability (\citet{leung2015testing})

\begin{itemize}
\tightlist
\item
  Variability-aware testing.
\item
  System integration testing has to be manually executed to evaluate the
  system's compliance with its specified requirement and performance.
\item
  Aids testers and developers to reduce their product time-to-market by
  utilizing historical testing results and similarity among systems.
\end{itemize}

\subsection{Test generation}\label{test-generation}

Scaling up automated test generation: Automatically generating
maintainable regression unit tests for programs (\citet{robinson2011})

\begin{itemize}
\tightlist
\item
  A system that has good coverage and mutation kill score, made readable
  code and required few edits as the system under test evolved. (stable)
\item
  Statement: The costs of unit tests are not perceived to outweigh the
  benefits.
\item
  Previous techniques: hard to understand / maintain / brittle and only
  tested on libraries → not real software development code.
\item
  They claim they made a pretty well working test-generation tool.
\end{itemize}

Obsidian: Pattern-Based Unit Test Implementations
(\citet{bowring2014obsidian})

\begin{itemize}
\tightlist
\item
  A tool that generates the templates for tests: guarantee compilation,
  support exception handling, find suitable location\ldots{} etc.
\item
  Developers still need to fix the oracle tests, but the
  implementation/template is there.
\item
  Looks at the context in order to decide what template to use.
\item
  Distinguishes implementations from test cases
\end{itemize}

How Do Automatically Generated Unit Tests Influence Software
Maintenance? (\citet{shamshiri2018automatically})

\begin{itemize}
\tightlist
\item
  Automatically generated tests are usually not based on realistic
  scenarios, and are therefore generally considered to be less readable.
\item
  Every time a test fails, a developer has to decide whether this
  failure has revealed a regression fault in the program under test, or
  whether the test itself needs to be updated.
\item
  Whilst maintenance activities take longer when working with
  automatically generated tests, they found developers to be equally
  effective with manually written and automatically generated tests.
\item
  There is a need for research into the generation of more realistic
  tests.
\end{itemize}

Model-Based Strategies for Reducing the Complexity of Statistically
Generated Test Suites (\citet{dulz2013model})

\begin{itemize}
\tightlist
\item
  By directed adjusting specific probability values in the usage profile
  of a Markov chain usage model it is relatively easy to generate
  abstract test suites for different user classes and test purposes in
  an automated approach.
\item
  By using proper tools, like the TestUS Testplayer even less
  experienced test engineers will be able to efficiently generate
  abstract test cases and to graphically assess quality characteristics
  of different test suites.
\end{itemize}

\subsection{Testing Practices}\label{testing-practices}

A survey of software testing practices in Canada
(\citet{GAROUSI20131354})

\begin{itemize}
\tightlist
\item
  The importance of testing-related training is increasing
\item
  Functional and unit-testing receive the most effort and attention
\item
  The mutation testing approach is getting attention amongst Canadian
  firms
\item
  Test last approach is still dominant, few companies try TDD
\item
  In terms of popularity: NUnit and Web application testing overtook
  JUnit and IBM Rational tools
\item
  Coverage metrics, to most commonly used: branch and conditional
  coverage
\item
  Number of passing test / defects per day is used as the most popular
  metric in order to determine a release
\item
  Ratio of testers : developers is somewhere around 1:2 and 1:5. The
  total effort is estimated to be less than 40\%
\item
  More than 70\% of the respondents participated in a forum related to
  testing on a regular basis
\item
  In general: more attention to testing (in 2012)
\end{itemize}

Developer testing in the IDE: Patterns, beliefs and behavior
(\citet{beller2017developer})

\begin{itemize}
\tightlist
\item
  Java C\# developer testing behavior
\item
  Little support for TDD
\item
  Developers execute tests phased
\item
  Only half of the developers practice testing actively
\item
  Testing time is overestimated twofold.
\item
  12\% of the test cases show flaky behavior
\item
  Correlation between test flakiness and CI error-proneness?
\item
  Few (25\%) tests detect 75\% of the execution failures.
\item
  Tests and production code do not co-evolve gracefully.
\end{itemize}

When, how, and why developers (do not) test in their IDEs
(\citet{beller2015})

\begin{itemize}
\tightlist
\item
  Developers largely do not run tests in the IDE. However, when they do,
  they do it heftily.
\item
  Tests run in the IDE take a short amount of time
\item
  Developers run selective tests (often 1)
\item
  Most test executions fail
\item
  A typical reaction is to dive into offending code
\item
  TDD is not widely practiced, even by those who say they do (strict
  definition)
\item
  The way people test is different from how they believe they test.
\end{itemize}

Uncertainty in Software Testing (\citet{moiz2017uncertainty})

\begin{itemize}
\tightlist
\item
  Mechanisms are needed to address uncertainty in each of the
  deliverables produced during software development process. The
  uncertainty metrics can help in assessing the degree of uncertainty.
\end{itemize}

\subsection{Risk-driven testing}\label{risk-driven-testing}

Investigating NLP-Based Approaches for Predicting Manual Test Case
Failure (\citet{hemmati2018})

\begin{itemize}
\tightlist
\item
  System-level manual acceptance testing is one of the most expensive
  testing activities.
\item
  A new test case failure prediction approach is proposed, which does
  not rely on source code or specification of the software under test.
\item
  The approach uses basic Information Retrieval (IR) methods on the test
  case descriptions, written in natural language, based on the frequency
  of terms in the manual test scripts.
\item
  The test fail prediction is accurate and the NLP-based feature can
  improve the prediction models.
\item
  ``To the best of our knowledge, this work is the first use of NLP on
  manual test case scripts for test failure prediction and has shown
  promising results, which we are planning to replicate on different
  systems and expand on different NLP-based features to more accurately
  extract features keywords from test cases.''
\end{itemize}

Risk-driven software testing and reliability (\citet{schneidewind2007})

This paper is discarded from the survey, because it uses weak models to
validate the claims made and is too old based on our exclusion criteria.

Risk-driven vulnerability testing: Results from eHealth experiments
using patterns and model-based approach (\citet{vernotte2015})

\begin{itemize}
\tightlist
\item
  This paper introduces and reports on an original tooled risk-driven
  security testing process called Pattern-driven and Model-based
  Vulnerability Testing. This fully automated testing process, drawing
  on risk-driven strategies and Model-Based Testing (MBT) techniques,
  aims to improve the capability of detection of various Web application
  vulnerabilities, in particular SQL injections, Cross-Site Scripting,
  and Cross-Site Request Forgery.
\item
  An empirical evaluation, conducted on a complex and freely-accessible
  eHealth system developed by Info World, shows that this novel process
  is appropriate for automatically generating and executing risk-driven
  vulnerability test cases and is promising to be deployed for
  large-scale Web applications.
\end{itemize}

A comparative study of software testing techniques (\citet{atifi2017})

\begin{itemize}
\tightlist
\item
  They highlight two software testing techniques considered among the
  most used techniques to perform software tests, and then perform a
  comparative study of these techniques, the approaches that support
  studied techniques, and the tools used for each technique.
\item
  The first technique is Model-based-testing, the second Risk-based
  testing.
\end{itemize}

Test case analytics: Mining test case traces to improve risk-driven
testing (\citet{noor2015test})

\begin{itemize}
\tightlist
\item
  In risk-driven testing, test cases are generated and/or prioritized
  based on different risk measures. *The most basic risk measure would
  analyze the history of the software and assigns higher risk to the
  test cases that used to detect bugs in the past.
\item
  A new risk measure is defined which assigns a risk factor to a test
  case, if it is similar to a failing test case from history. *The new
  risk measure is by far more effective in identifying failing test
  cases compared to the traditional risk measure.
\item
  ``Though our initial and simple implementation in this paper was very
  promising, we are planning to investigate other similarity functions,
  specifically those that account for the method orders in the trace. In
  addition, this project is a sub-project of a bigger research on
  risk-driven model-based testing, where we are planning to extract
  specification models of the system and augment them with the
  similarity-based risk measures. Those models can later be used in both
  risk-driven test generation and prioritization.''
\end{itemize}

\chapter{Build analytics}\label{build-analytics}

\section{Motivation}\label{motivation-1}

Ideally, when building a project from source code to executable, the
process should be fast and without any errors. Unfortunately, this is
not always the case and automated builds results notify developers of
compile errors, missing dependencies, broken functionality and many
other problems. This chapter is aimed to give an overview of the effort
made in build analytics field and Continuous Integration (CI) as an
increasingly common development practice in many projects.

\section{Research Questions}\label{research-questions}

\begin{itemize}
\tightlist
\item
  \textbf{RQ1} What is the current state of the art in the field of
  build analytics? 
\item
  \textbf{RQ2} What is the current state of practice in the field of
  build analytics? 
\item
  \textbf{RQ3} What future research can we expect in the field of build
  analytics? 
\end{itemize}

\section{Research protocol}\label{research-protocol-1}

Using the initial seed consisting of \citet{bird2017predicting},
\citet{beller2017oops}, \citet{rausch2017empirical},
\citet{beller2017travistorrent}, \citet{pinto2018work},
\citet{zhao2017impact}, \citet{widder2018m} and \citet{hilton2016usage}
we used references to find new papers to analyze. Moreover, we used
academical search engines like \emph{GoogleScholar} to perform a keyword
based search for other relevant build analytics domain papers. The
keywords used were: build analytics, machine learning, build time,
prediction, continuous integration, build failures, active learning,
build errors, mining, software repositories, open-source software.

\section{Answers}\label{answers}

Through this we found the following papers

\section{Summary of papers}\label{summary-of-papers}

\subsection{\texorpdfstring{\citet{bird2017predicting}}{@bird2017predicting}}\label{bird2017predicting}

\emph{Initial Seed}

This is a US patent grant for a method of predicting software build
errors. This patent is owned by Microsoft. Using logistic regression a
prediction can be made on the probability of a build failing. Using this
method build errors can be better anticipated, which decreases the time
until the build works again.

\subsection{\texorpdfstring{\citet{beller2017oops}}{@beller2017oops}}\label{beller2017oops}

\emph{Initial Seed}

This paper explores data from Travis CI\footnote{See
  \url{https://travis-ci.org}} on a large scale by analyzing 2,640,825
build logs of Java and Ruby builds. It uses \textsc{TravisTorrent} as a
data source. It is found that the number one reason for failing builds
it test failure. It also explores differences in testing between Java
and Ruby.

\subsection{\texorpdfstring{\citet{rausch2017empirical}}{@rausch2017empirical}}\label{rausch2017empirical}

\emph{Initial Seed}

A stuy on the build results of 14 open source software Java projects. It
is similar to \citet{beller2017oops}, albeit on a smaller scale. It does
go more in depth on the result and changes over time.

\subsection{\texorpdfstring{\citet{beller2017travistorrent}}{@beller2017travistorrent}}\label{beller2017travistorrent}

\emph{Initial Seed}

This paper introduces \textsc{TravisTorrent}, a dataset containing
analyzed builds from more than 1,000 projects. This data is freely
downloadable from the internet. It uses \textsc{GHTorrent} to link the
information from travis to commits on GitHub.

\subsection{\texorpdfstring{\citet{pinto2018work}}{@pinto2018work}}\label{pinto2018work}

\emph{Initial Seed}

This paper is a survey amongst Travis CI users. It found that users are
not sure whether a job failure represents a failure or not, that
inadequate testing is the most common (technical) reason for build
breakage and that people feel that there is a false sense of confidence
when blindly trusing tests.

\subsection{\texorpdfstring{\citet{zhao2017impact}}{@zhao2017impact}}\label{zhao2017impact}

\emph{Initial Seed}

This paper analyzed approximately 160,000 projects written in seven
different programming languages. It notes that adoption of CI is often
part of a reorganization. It collected information on the differences
before and after adoption of CI. There is also a survey amongst
developers to learn about their experiences in adopting Travis CI.

\subsection{\texorpdfstring{\citet{widder2018m}}{@widder2018m}}\label{widder2018m}

\emph{Initial Seed}

This paper analyzes what factors have impact on abandonment of Travis.
They find that increased build complexity reduces the chance of
abandonment, but larger projects abandon at a higher rate and that a
project's language has significant but varying effect. A surprising
result is that metrics of configuration attempts and knowledge
dispersion in the project do not affect the rate of abandonment.

\subsection{\texorpdfstring{\citet{hilton2016usage}}{@hilton2016usage}}\label{hilton2016usage}

\emph{Initial Seed}

This paper explores which CI system developers use, how developers use
CI and why developers use CI. For this it analyzes data from Github,
Travis CI and it conducts a developer survey. It finds that projects
using CI release twice as often, accept pull requests faster and have
developers who are less worried about breaking the build.

\subsection{\texorpdfstring{\citet{vassallo2017tale}}{@vassallo2017tale}}\label{vassallo2017tale}

\emph{References \citet{beller2017oops} }

This paper discusses the difference in failures on continuous
integration between open source software (OSS) and industrial software
projects. For this 349 Java OSS projects and 418 project from ING
Nederland, a financial organization.

Using cluser analysis it was observed that both kinds of projects share
similar build failures, but in other cases very different patterns
emerge.

\subsection{\texorpdfstring{\citet{hassan2018hirebuild}}{@hassan2018hirebuild}}\label{hassan2018hirebuild}

\emph{References \citet{beller2017travistorrent} }

This paper uses TravisTorrent (\citet{beller2017travistorrent}) to show
that 22\% of code commits include changes in build script files to keep
the build working or to fix the build.

In the paper a tool is proposed to automatically fix build failures
based on previous changes.

\subsection{\texorpdfstring{\citet{vassallo2018break}}{@vassallo2018break}}\label{vassallo2018break}

\emph{References \citet{beller2017oops}, \citet{rausch2017empirical} }

This paper proposes a tool called \textsc{BART} to help developers fix
build errors. This tool eliminates the need to browse error logs which
can be very long by generating a summary of the failure with useful
information.

\subsection{\texorpdfstring{\citet{zampetti2017open}}{@zampetti2017open}}\label{zampetti2017open}

\emph{Referenced by \citet{vassallo2018break} }

This paper studies the usage of static analysis tools in 20 Java open
source software projects hosted on GitHub and using Travic CI as
continuous integration infrastructure. There is investigated which tools
are being used, what types of issues make the build fail or raise
warnings and how is responded to broken builds.

\chapter{Sample Sub-Topic}\label{sample-sub-topic}

\emph{This is an example for the deliverable every group works on. Every
group works on one independent chapter (starting as one Rmd file).}

\section{Motivation}\label{motivation-2}

\emph{A short introduction about why the topic you are working on is
interesting.}

The RQs that everyone should be aiming at are:

\begin{itemize}
\tightlist
\item
  \textbf{RQ1} Current state of the art in software analytics for
  \emph{your topic }:

  \begin{itemize}
  \tightlist
  \item
    Topics that are being explored
  \item
    Research methods, tools and datasets being used
  \item
    Main research findings, aggregated
  \end{itemize}
\item
  \textbf{RQ2} Current state of practice in software analytics for
  \emph{your topic }:

  \begin{itemize}
  \tightlist
  \item
    Tools and companies creating / employing them
  \item
    Case studies and their findings
  \end{itemize}
\item
  \textbf{RQ3} Open challenges and future research required
\end{itemize}

\section{Research protocol}\label{research-protocol-2}

\emph{Here, you describe the details of applying Kitchenham's survey
method for your topic, including search queries, fact extraction, coding
process and an intial groupping of the papers that you will be
analyzing.}

\section{Answers}\label{answers-1}

\emph{Aggregated answers to the RQs, per RQ. You need:}

\begin{itemize}
\tightlist
\item
  For \textbf{RQ1}

  \begin{itemize}
  \tightlist
  \item
    Topics that are being explored
  \item
    Research methods, tools and datasets being used
  \item
    Main research findings, aggregated
  \end{itemize}
\item
  For \textbf{RQ2} :

  \begin{itemize}
  \tightlist
  \item
    Tools and companies creating / employing them
  \item
    Case studies and their findings
  \end{itemize}
\item
  For \textbf{RQ3}:

  \begin{itemize}
  \tightlist
  \item
    List of challenges
  \item
    An aggregated set of open research items, as described in the papers
  \item
    Research questions that emerge from the synthesis of the presented
    works
  \end{itemize}
\end{itemize}

\chapter{Sample Sub-Topic}\label{sample-sub-topic-1}

\emph{This is an example for the deliverable every group works on. Every
group works on one independent chapter (starting as one Rmd file).}

\section{Motivation}\label{motivation-3}

\emph{A short introduction about why the topic you are working on is
interesting.}

The RQs that everyone should be aiming at are:

\begin{itemize}
\tightlist
\item
  \textbf{RQ1} Current state of the art in software analytics for
  \emph{your topic }:

  \begin{itemize}
  \tightlist
  \item
    Topics that are being explored
  \item
    Research methods, tools and datasets being used
  \item
    Main research findings, aggregated
  \end{itemize}
\item
  \textbf{RQ2} Current state of practice in software analytics for
  \emph{your topic }:

  \begin{itemize}
  \tightlist
  \item
    Tools and companies creating / employing them
  \item
    Case studies and their findings
  \end{itemize}
\item
  \textbf{RQ3} Open challenges and future research required
\end{itemize}

\section{Research protocol}\label{research-protocol-3}

\emph{Here, you describe the details of applying Kitchenham's survey
method for your topic, including search queries, fact extraction, coding
process and an intial groupping of the papers that you will be
analyzing.}

\section{Answers}\label{answers-2}

\emph{Aggregated answers to the RQs, per RQ. You need:}

\begin{itemize}
\tightlist
\item
  For \textbf{RQ1}

  \begin{itemize}
  \tightlist
  \item
    Topics that are being explored
  \item
    Research methods, tools and datasets being used
  \item
    Main research findings, aggregated
  \end{itemize}
\item
  For \textbf{RQ2} :

  \begin{itemize}
  \tightlist
  \item
    Tools and companies creating / employing them
  \item
    Case studies and their findings
  \end{itemize}
\item
  For \textbf{RQ3}:

  \begin{itemize}
  \tightlist
  \item
    List of challenges
  \item
    An aggregated set of open research items, as described in the papers
  \item
    Research questions that emerge from the synthesis of the presented
    works
  \end{itemize}
\end{itemize}

\chapter{Release Engineering
Analytics}\label{release-engineering-analytics}

\section{Search Strategy}\label{search-strategy}

Release engineering is a relatively new research topic, given that
modern processes for releasing software (e.g.~continuous delivery) are
industry-driven. Therefore, we took an exploratory approach in
collecting any literature revolving around the topic of release
engineering from the perspective of software analytics. This will aid us
in determining a more narrow scope for our survey, subsequently allowing
us to find additional literature fitting this scope.

At the start of this project, five papers were given to us as a starting
point for the literature survey. These initial papers were
\citet{adams2016a}, \citet{da2016a}, \citet{d2014a}, \citet{khomh2012a},
and \citet{khomh2015a}.

We collected publications using two search engines: Scopus and Google
Scholar. These each encompass various databases such as ACM Digital
Library, Springer, IEEE Xplore and ScienceDirect. The queries we entered
are summarized in Figure 1. The publications found using this query
were:

\begin{itemize}
\tightlist
\item
  \citet{kaur2019a}
\item
  \citet{kerzazi2013a}
\item
  \citet{castelluccio2017a}
\item
  \citet{karvonen2017a}
\item
  \citet{claes2017a}
\item
  \citet{fujibayashi2017a}
\item
  \citet{souza2015a}
\item
  \citet{laukkanen2018a}
\end{itemize}

\begin{verbatim}
TITLE-ABS-KEY(
  (
    "continuous release" OR "rapid release" OR "frequent release"
    OR "quick release" OR "speedy release" OR "accelerated release"
    OR "agile release" OR "short release" OR "shorter release"
    OR "lightning release" OR "brisk release" OR "hasty release"
    OR "compressed release" OR "release length" OR "release size"
    OR "release cadence" OR "release frequency"
    OR "continuous delivery" OR "rapid delivery" OR "frequent delivery"
    OR "fast delivery" OR "quick delivery" OR "speedy delivery"
    OR "accelerated delivery" OR "agile delivery" OR "short delivery"
    OR "lightning delivery" OR "brisk delivery" OR "hasty delivery"
    OR "compressed delivery" OR "delivery length" OR "delivery size"
    OR "delivery cadence" OR "continuous deployment" OR "rapid deployment"
    OR "frequent deployment" OR "fast deployment" OR "quick deployment"
    OR "speedy deployment" OR "accelerated deployment" OR "agile deployment"
    OR "short deployment" OR "lightning deployment" OR "brisk deployment"
    OR "hasty deployment" OR "compressed deployment" OR "deployment length"
    OR "deployment size" OR "deployment cadence"
  ) AND (
    "release schedule" OR "release management" OR "release engineering"
    OR "release cycle" OR "release pipeline" OR "release process"
    OR "release model" OR "release strategy" OR "release strategies"
    OR "release infrastructure"
  )
  AND software
) AND (
    LIMIT-TO(SUBJAREA, "COMP") OR LIMIT-TO(SUBJAREA, "ENGI")
)
AND PUBYEAR AFT 2014
\end{verbatim}

\emph{Figure 1. Query used for retrieving release engineering
publications via Scopus.}

In addition to the search strategy that is based on a combination of
keywords and subject headings as described above, a review of the list
of publications that retrieved papers cite and are cited by is done.
These lists are provided by Google Scholar, as well as the reference
lists of the papers themselves. Results of which are listed in Table 1.

\emph{Table 1. Papers found indirectly by investigating citations of/by
other papers.}

\begin{longtable}[]{@{}lll@{}}
\toprule
Starting point & Type & Result\tabularnewline
\midrule
\endhead
\citet{souza2015a} & has cited & \citet{plewnia2014a}
\citet{mantyla2015a}\tabularnewline
\citet{khomh2015a} & is cited by & \citet{poo-caama2016a}
\citet{teixeira2017a}\tabularnewline
\citet{mantyla2015a} & is cited by & \citet{rodriguez2017a}
\citet{cesar2017a}\tabularnewline
\bottomrule
\end{longtable}

In order to aggregate our collective efforts, selected sources were
stored in a custom built web-based tool for conducting literature
reviews. The source code of this tool is published in a
\href{https://github.com/jessetilro/research}{GitHub repository}. By
commenting on and tagging our findings we were able to export a filtered
list of publications, the relevance of which was agreed upon.

\chapter{Sample Sub-Topic}\label{sample-sub-topic-2}

\emph{This is an example for the deliverable every group works on. Every
group works on one independent chapter (starting as one Rmd file).}

\section{Motivation}\label{motivation-4}

\emph{A short introduction about why the topic you are working on is
interesting.}

The RQs that everyone should be aiming at are:

\begin{itemize}
\tightlist
\item
  \textbf{RQ1} Current state of the art in software analytics for
  \emph{your topic }:

  \begin{itemize}
  \tightlist
  \item
    Topics that are being explored
  \item
    Research methods, tools and datasets being used
  \item
    Main research findings, aggregated
  \end{itemize}
\item
  \textbf{RQ2} Current state of practice in software analytics for
  \emph{your topic }:

  \begin{itemize}
  \tightlist
  \item
    Tools and companies creating / employing them
  \item
    Case studies and their findings
  \end{itemize}
\item
  \textbf{RQ3} Open challenges and future research required
\end{itemize}

\section{Research protocol}\label{research-protocol-4}

\emph{Here, you describe the details of applying Kitchenham's survey
method for your topic, including search queries, fact extraction, coding
process and an intial groupping of the papers that you will be
analyzing.}

\section{Answers}\label{answers-3}

\emph{Aggregated answers to the RQs, per RQ. You need:}

\begin{itemize}
\tightlist
\item
  For \textbf{RQ1}

  \begin{itemize}
  \tightlist
  \item
    Topics that are being explored
  \item
    Research methods, tools and datasets being used
  \item
    Main research findings, aggregated
  \end{itemize}
\item
  For \textbf{RQ2} :

  \begin{itemize}
  \tightlist
  \item
    Tools and companies creating / employing them
  \item
    Case studies and their findings
  \end{itemize}
\item
  For \textbf{RQ3}:

  \begin{itemize}
  \tightlist
  \item
    List of challenges
  \item
    An aggregated set of open research items, as described in the papers
  \item
    Research questions that emerge from the synthesis of the presented
    works
  \end{itemize}
\end{itemize}

\chapter{Sample Sub-Topic}\label{sample-sub-topic-3}

\emph{This is an example for the deliverable every group works on. Every
group works on one independent chapter (starting as one Rmd file).}

\section{Motivation}\label{motivation-5}

\emph{A short introduction about why the topic you are working on is
interesting.}

The RQs that everyone should be aiming at are:

\begin{itemize}
\tightlist
\item
  \textbf{RQ1} Current state of the art in software analytics for
  \emph{your topic }:

  \begin{itemize}
  \tightlist
  \item
    Topics that are being explored
  \item
    Research methods, tools and datasets being used
  \item
    Main research findings, aggregated
  \end{itemize}
\item
  \textbf{RQ2} Current state of practice in software analytics for
  \emph{your topic }:

  \begin{itemize}
  \tightlist
  \item
    Tools and companies creating / employing them
  \item
    Case studies and their findings
  \end{itemize}
\item
  \textbf{RQ3} Open challenges and future research required
\end{itemize}

\section{Research protocol}\label{research-protocol-5}

\emph{Here, you describe the details of applying Kitchenham's survey
method for your topic, including search queries, fact extraction, coding
process and an intial groupping of the papers that you will be
analyzing.}

\section{Answers}\label{answers-4}

\emph{Aggregated answers to the RQs, per RQ. You need:}

\begin{itemize}
\tightlist
\item
  For \textbf{RQ1}

  \begin{itemize}
  \tightlist
  \item
    Topics that are being explored
  \item
    Research methods, tools and datasets being used
  \item
    Main research findings, aggregated
  \end{itemize}
\item
  For \textbf{RQ2} :

  \begin{itemize}
  \tightlist
  \item
    Tools and companies creating / employing them
  \item
    Case studies and their findings
  \end{itemize}
\item
  For \textbf{RQ3}:

  \begin{itemize}
  \tightlist
  \item
    List of challenges
  \item
    An aggregated set of open research items, as described in the papers
  \item
    Research questions that emerge from the synthesis of the presented
    works
  \end{itemize}
\end{itemize}

\chapter{App Store analytics}\label{app-store-analytics}

\section{API change and fault proneness: A threat to the success of
Android
apps}\label{api-change-and-fault-proneness-a-threat-to-the-success-of-android-apps}

M. Linares-Vásquez, G. Bavota, C. Bernal-Cárdenas, M. Di Penta, R.
Oliveto, and D. Poshyvanyk, in Proceedings of the 2013 9th joint meeting
on foundations of software engineering, 2013, pp. 477--487.

The paper presents an empirical study that aims to corroborate the
relationship between the fault and change-proneness of APIs and the
degree of success of Android apps measured by their user ratings. For
this, the authors selected a sample of 7,097 free Android apps from the
Google Play Market and gathered information of the changes and faults
that the APIs used by them presented. Using this data and statistical
tools such as box-plots and the Mann-Whitney test, two main hypotheses
were analyzed. The first hypothesis tested the relationship between
fault-proneness (number of bugs fixed in the API) and the success of an
app. The second tested the relationship between change-proneness
(overall method changes, changes in method signatures and changes to the
set of exceptions thrown by methods) and the success of an app. Finally,
although no causal relationships between the variables can be assumed,
the paper found significant differences of the level of success of the
apps taking into consideration the change and fault-proneness of the
APIs they use.

\section{The Impact of API Change and Fault-Proneness on the User
Ratings of Android
Apps}\label{the-impact-of-api-change-and-fault-proneness-on-the-user-ratings-of-android-apps}

G. Bavota, M. Linares-Vásquez, C. E. Bernal-Cárdenas, M. D. Penta, R.
Oliveto and D. Poshyvanyk, in IEEE Transactions on Software Engineering,
vol.~41, no. 4, pp.~384-407, 1 April 2015. doi: 10.1109/TSE.2014.2367027

The paper by Bavota et al. aims to find empirical evidence supporting
the success of apps and the relationship with change- and
fault-proneness of the underlying APIs, where the success of the app is
measured by its user rating. They performed two case studies to find
quantitative evidence using 5848 free Android apps as well as an
explanation for these results doing a survey with 45 professional
Android developers. The quantitative case study was done by comparing
the user ratings to the number of bug fixes and changes in the API that
an app uses. They found that apps with a high user rating are
significantly less change- and fault-prone than APIs used by apps with a
low user rating. In the second case study the paper found that most of
the 45 developers observed a direct relationship between the user
ratings of apps and the APIs those apps use.

\section{How can i improve my app? Classifying user reviews for software
maintenance and
evolution}\label{how-can-i-improve-my-app-classifying-user-reviews-for-software-maintenance-and-evolution}

S. Panichella, A. D. Sorbo, E. Guzman, C. A. Visaggio, G. Canfora, and
H. C. Gall, in 2015 ieee international conference on software
maintenance and evolution (icsme), 2015, pp.~281--290.

The most popular apps in the app stores (such as Google Play or App
Store) receive thousands of user reviews per day and therefore it would
be very time demanding to go through the reviews manually to obtain
relevant information for the future development of the apps. This paper
uses a combination of Natural Language Processing Sentiment Analysis and
Text Analysis to extract relevant sentences from the reviews and to
classify them into the following categories: Information Seeking,
Information Giving, Feature Request, Problem Discovery, and Others. The
results show 75\% precision and 74\% recall when classifier (J48 using
data from NLP+SA+TA) is trained on 20\% of the data (1421 manually
labeled sentences from reviews of seven different apps) and the rest is
used for testing. The paper also states that the results do not differ
in a statistically significant manner when a different classifier is
used and shows that precision and recall can be further improved by
increasing the size of the data set.

\chapter{Final Words}\label{final-words}

We have finished a nice book on Software Analytics.

\bibliography{book.bib}


\end{document}
